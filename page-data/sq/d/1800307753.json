{"data":{"allContentfulBlogPost":{"nodes":[{"id":"cf18d799-e8cd-5273-a427-8c4e365bfeb6","date":"January 27th, 2025","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"When it comes to automated testing, one of the most common challenges teams face is effectively visualizing test results. Traditional methods often involve sifting through endless logs, spreadsheets, and static reports, which can be time-consuming and error prone. This complexity makes it difficult for teams to quickly identify trends, pinpoint issues, and make data-driven decisions.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Why use ReportPortal for test results?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal addresses these challenges by providing a centralized platform for managing and visualizing test results. With its intuitive dashboard and real-time analytics, ReportPortal enables teams to gain insights into their testing processes and make informed decisions.\\n\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.youtube.com/watch?v=CMDa5ZUUOh0\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Watch our video about test results visualization\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nBy incorporating AI-driven features, ReportPortal enhances the efficiency and effectiveness of analyzing test results. These features introduce an intelligent, automated approach to defining test failure reasons.\\n\\nA closer examination reveals how ReportPortal delivers deeper insights through test result visualization. The more consistent and well-prepared test results are when submitted to ReportPortal, the more informative and accurate its analytics become. For this reason, following \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"best practices in reporting\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" is crucial to achieving better visualization.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Use Consistent Launch Names\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Leverage Attributes\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Reporting best practices for greater visualization\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Using consistent Launch names\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Imagine a team running end-to-end (E2E) tests. Each test execution creates a new launch in ReportPortal. To ensure the \\\"Latest launches\\\" feature works effectively, the launch names should remain consistent across runs. Avoid dynamic values like timestamps or build numbers in launch names – instead, use attributes for these details.\\n\\nWhy does this matter? Maintaining consistent launch names is crucial for effectively utilizing the \\\"Latest launches\\\" feature, which displays only the most recent launches based on their names. It also facilitates tracking the history of a test case through the same launch, in addition to viewing it across all launches.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://demo.reportportal.io/ui/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal's demo instance\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" demonstrates these capabilities. By signing up through GitHub, users can explore how features like \\\"Latest launches\\\" streamline test tracking. For instance, four launches – two for E2E tests and two for Unit tests – can be reported.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"7FIIAWr649RGA9N7NAMvWp\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Using the \\\"Latest launches\\\" control, users can focus only on the most recent executions for each launch name. This feature simplifies tracking when numerous launches are reported, and it can also be used to configure widgets.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"73SJR4gMuaSpy0hKb4ziMO\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal's Analyzer can also be configured to work with launches that share consistent names, enabling more targeted decision-making. Additionally, Notification and Quality Gate rules should be set up for launches with specific names.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5UobElGNtxM3UlIT4KJQMC\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Leveraging attributes\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal attributes is a powerful feature that allows users to add metadata to their test results in a structured way. Like tags in automation frameworks, attributes in ReportPortal are used to categorize and filter test data, but they offer a bit more flexibility and detail. Attributes in ReportPortal are defined in a \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"key:value\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" format, which allows for more precise classification and retrieval of test information.\\n\\nA small project with E2E and Unit tests written using Playwright and Node.js can serve as an example for updating the ReportPortal configuration with meaningful attributes. The product type, such as \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"app: shop\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", can be predefined, with additional dynamic values – like \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"build number, platform, branch, sprint\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" – being derived from environment variables.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"31YXJQ7jiBcUk9LTrUqAbQ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6I5vBB3aLCVhePX1CaApaU\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Consider a scenario where different teams contribute their tests to a single repository, and these tests are executed in a collective suite and reported as a single launch in ReportPortal.\\n\\nIn this case it would be helpful to attach attributes at the test level to later distinguish which team the tests belong to. Add t\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"eam, feature, priority, scenario\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" attributes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3wwpJCY92I8U7WxJLKhDGU\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For users working with Playwright or Cypress, there are separate videos demonstrating how to include attributes and additional data in test results: one for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.youtube.com/watch?v=UdeRKqFVcuU\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Playwright\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" and another for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.youtube.com/watch?v=BNQpsVDMQ4E\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Cypress\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". Additionally, there is separate documentation for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/log-data-in-reportportal/test-framework-integration/JavaScript/Playwright/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Playwright\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" and \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/log-data-in-reportportal/test-framework-integration/JavaScript/Cypress/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Cypress,\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" as well as for other \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/log-data-in-reportportal/test-framework-integration/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"test frameworks\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\\n\\nAfter adding attributes and running tests, the ReportPortal UI displays these attributes. To check the test attributes, we need to drill down to the launch. They are presented under the test name and can also be used for filtering the list.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"798e2w3PUJsjXpC4gkwIZQ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"At the launch level, attributes can be used to filter the launch list directly.\\n\\nTo use these attributes effectively, saved searches for launches, called \\\"Filters\\\", can be created in ReportPortal. For instance, if you want information specifically about the E2E tests, create a filter for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"E2E shop tests\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1mel2VlhSuRHwoFsabYveu\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To gather a broader overview, create another filter that displays all the launches for our application, simply by using our common attribute \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"app: shop\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" and name it as \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"All shop tests\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"C1pcI6XFGTOCMrZJgU7Sp\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"7uWjt7JEXfpSGPJHblYB8E\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to visualize test results with attributes\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"After these preparations, widgets can be created to visualize the data.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Component Health Check widget\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, Component Health Check and his big brother – Component Health Check (table view). \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/ComponentHealthCheck/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Component health check widget\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" shows the passing rate of the application components, indicated by the attributes specified for test cases. Any attributes can be used to build the structure that addresses your needs.\\n\\nYou can create a widget for each existing filter right on the Launches page.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6z7KLsB86OurwelFyU5q6B\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"As one example, the E2E filter can be selected to create the Component Health Check widget based on it. Filter conditions can be edited directly if changes are needed in the selection of launches. The option \\\"Latest launches\\\" can be applied to focus the widget on the most recent results.\\n\\nAdditional levels, such as \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"feature, priority, platform, team\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", and \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"scenario\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", can be added. If no dashboard exists for selection, a new one will be created. The widget can then be named \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"E2E tests by feature\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5URBNk2YmH0yGbZUznElhx\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"272w7h5392A8YxZ7J1ZjqU\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The newly created dashboard contains aggregated test cases from all launches in the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"E2E filter\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", grouped by attribute values. All \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"features\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" are displayed, and selecting one allows viewing the tests' passing rates for different \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"priorities\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"2pQyVojE3j4aZZEBvZ3Hu5\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1mkEpT7a0bFtH8fmybbbM5\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Further exploration reveals execution results for each \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"platform\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4mCAHIdayfML3Z4UndqNRY\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If identifying the responsible \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"team\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" is necessary, deeper levels can be accessed.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4kjRLPZtkaiztDV8zFlBDz\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Once the target group is located, the test results list can be accessed for further investigation by clicking the \\\"Open list button\\\". This option is available from any level.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"2R8uDeC36dLsJRcP1sC1mQ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5NAYlGQcukrlPMN1w7kioH\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This process demonstrates how any structure can be built with the Component Health Check widget by using correctly specified attributes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Component Health Check table view widget\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Next, let’s build \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/TableComponentHealthCheck/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Component Health Check table view widget\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". It presents detailed statistics of the application components which are indicated by the specified attributes. Although it is like the previous widget, this one shows more data, including issue statistics.\\n\\nFor this widget, the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"All shop tests\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" filter is used. The \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Latest launches\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" option is selected, along with the following levels: \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"build, platform, team\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\". It is possible to mix attributes from both the launch and test levels, as launch-level attributes are automatically propagated by ReportPortal to all its children.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1Tfne6Cycly8VVTdjPaxu\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Once the widget is created, additional time may be required to process a large amount of data. Afterward, it becomes possible to drill down to the next levels, revealing grouped test results and additional information about statistics and defects.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6JAEforTtHcKOKBFdpdtp\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Both Component Health checks are extremely powerful and can cover many of your needs, but let's look at the other useful widgets that ReportPortal offers.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Widgets by purpose\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Analytics for a single test run\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Gain actionable insights into individual test runs with these three essential widgets, designed to highlight key metrics such as pass rates, flaky tests, and time-consuming cases.\\n\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/PassingRatePerLaunch/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Passing rate per launch\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nWhile this widget is simple, it clearly provides main information about the results of the last run.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6ZGx47VvlbpDqw4fcvNgUP\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/FlakyTestCasesTableTop50/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Flaky test cases table\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nThis widget is useful for pinpointing unstable autotests that yield different statuses with each execution. Next, we need to consider: could the issue be with the autotest rather than the product itself? It is recommended to pay attention to flaky tests as they affect the stability of the test automation.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"7g1SifDuy8nCGCoEEqaxuu\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/MostTimeConsumingTestCasesWidgetTop20/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Most time-consuming test cases\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nIt's beneficial to use this in conjunction with the Launches duration chart: whenever we notice that a launch is taking an extended amount of time, we can build the Most Time-Consuming Test Cases widget by the name of this launch and identify which test cases are taking the longest and think about their optimization.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3sORuV5R4QJofAxqCVvoBs\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Aggregated analytics from several launches\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Explore key widgets that provide a comprehensive view of aggregated analytics across multiple launches, offering valuable insights into trends, growth, and overall statistics.\\n\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/CumulativeTrendChart/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Cumulative trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nYou can observe the changes in statistics from one build to another, or from one version to the next. Whenever a new version (build, release, or other) is added to ReportPortal, new data will be incorporated into the graph.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6OiBQR1gmkyCqVQ4usJATq\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/TestCasesGrowthTrendChart/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Test-cases growth trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nThis widget can also be valuable for monitoring the overall increase in test cases within a specific launch.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5meJ1AJfkVbgFqcaOp2Vgj\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/OverallStatistics/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Overall statistics\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\\n\\nThanks to this widget, you can look at the distribution of failure reasons.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"7HHszNVFlZ4CNlysKMHMx8\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"While this article highlighted 8 essential widgets, ReportPortal offers a total of \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"more than 20 widgets\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", each designed to cater to specific testing needs. Exploring the full range of widgets will help uncover additional opportunities to optimize test results visualization and streamline decision-making.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Test results visualization"},"leadParagraph":{"leadParagraph":"When it comes to automated testing, one of the most common challenges teams face is effectively visualizing test results. Traditional methods often involve sifting through endless logs, spreadsheets, and static reports, which can be time-consuming and error prone."},"category":["Best practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/1BHwqLqgoiRtpDcLrzqJ5y/5da8dddb9eee3dd05fcae368133cd885/TestResultsVisualization.png"}},"slug":"test-results-visualization"},{"id":"24566794-5d5d-57dc-af98-3e0748f8e1c7","date":"January 8th, 2025","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We’re excited to announce a significant improvement to our Helm Chart versioning: chart versions and product release versions are now managed separately. This change brings greater clarity and flexibility for DevOps teams working with ReportPortal. Going forward, the appVersion in the Chart.yaml file will reflect the product release version, ensuring a direct link to the ReportPortal release you’re deploying. Meanwhile, the version field will denote the Helm Chart’s own version, helping you track updates and changes specific to the chart itself.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This separation allows us to release updates to the Helm Chart more frequently, addressing functional enhancements and security fixes independently of ReportPortal product releases. By aligning product and chart versioning independently, we aim to simplify the deployment and maintenance of ReportPortal. Check out our updated \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://github.com/reportportal/kubernetes/releases\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Helm Chart repository\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" for the latest changes and let us know how this update improves your workflow!\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Simplified Helm Chart versioning for ReportPortal"},"leadParagraph":{"leadParagraph":"We’re excited to announce a significant improvement to our Helm Chart versioning: chart versions and product release versions are now managed separately."},"category":["Product"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/1H7d0FR84BAJzWZH00yBHY/2572baa2308802f499564497912234cb/HelmChart-icon.jpg"}},"slug":"simplified-helm-chart-versioning-for-reportportal"},{"id":"3a78ec05-86af-56c5-959e-1d27ee005d81","date":"December 19th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Test automation is now a key part of successful software development, helping companies release better products more quickly. While manual testers still play a crucial role, automation testing offers faster results, increased test coverage, and savings in manual test execution time. The number of automated test cases directly influences the time required for regression cycles and ultimately improves the time to market for a product. Setting up automation costs money at the start, so it’s important to calculate the Return on Investment (ROI) to check if the benefits are worth the expense. Integrating test automation reporting tools like ReportPortal can improve ROI by streamlining test failure analysis and providing deeper analytics.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What is the ROI of test automation?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ROI is a way to measure how much you gain compared to what you spend. For test automation, it shows the value automation adds to your testing process. Calculating ROI is important because:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Highlights cost savings: demonstrates how much time or money automation saves compared to manual testing.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Supports automation decisions: provides clear data to convince managers or investors.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Facilitates planning: offers insight into when returns can be expected, making budget and resource allocation more effective.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"By automating tests, businesses can test faster, reduce manual work, and improve the quality of their products. Automation of manual test cases allows teams to focus on complex scenarios while repetitive tasks are handled by automated test scripts, improving product quality and efficiency. ROI helps ensure that your automation efforts are worth the investment.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal can further improve ROI by automating result analysis, enabling quicker decision-making, and reducing manual effort in tracking and managing test results.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to calculate the ROI of test automation\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Figuring out ROI is a straightforward way to show automation’s value. The basic formula is:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ROI = (Benefits – Costs) / Costs * 100%\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Here’s how it works:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Benefits from test automation can include:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Saving money: automating tests means paying less for manual work.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Saving time: repeating the same tests takes much less time with automation.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Finding bugs earlier: fixing problems sooner means fewer expensive fixes later.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Quicker time-to-market: automation allows for more testing, so you can release updates faster.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Costs include:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Initial setup: buying tools and setting up your automation system.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Development time: writing and setting up test scripts.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Maintenance: keeping your tests updated as your software changes.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Efforts on test results analysis: time and resources used to analyze test failures and generate reports.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, if automating tests saves you $50,000 a year but costs $30,000 to set up and maintain, the ROI is:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ROI = (50,000 – 30,000) / 30,000 * 100% = 66.7%\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This demonstrates how effective automation can be in reducing costs and improving the time required for testing cycles. With ReportPortal, you can further increase these savings by automating result analysis and reducing the overhead of manual test report creation.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Common mistakes that impact ROI in test automation\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To get the most out of test automation, avoid these common mistakes:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Automating the wrong tests\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Some tests don’t need automation. Automating rarely used or overly complicated tests can waste money and time.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Skipping maintenance\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Automated tests need regular updates to work with changes in the software. Ignoring this can lead to broken tests and unreliable results.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Focusing only on costs\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Trying to cut costs too much can backfire. Instead, focus on the return on investment ROI in the long term, considering benefits like better product quality and faster releases.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"4. Not tracking results\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To evaluate the value of test automation, \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"it's essential to measure metrics\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" both before and after implementing it. This helps you clearly see the impact and identify areas of improvement.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Start by assessing your test coverage — both manual and automated — before automation. Then, measure your \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"automation rate\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", which ideally should reach around 80% of your test cases.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"After implementing test automation, focus on metrics like the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"time saved in regression testing\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\". For example, if regression used to take a week, automation could reduce it to just four days.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Another key factor in improving ROI is the significant \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"reduction in test result analysis time\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\". With ReportPortal’s real-time reporting capabilities, teams can start investigating the first failures the moment they appear, rather than waiting for the entire regression suite to complete. This approach shortens the feedback loop considerably, allowing you to address issues more efficiently.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Beyond real-time insights, ReportPortal’s automated analysis — powered by Machine Learning — further accelerates the process. Over time, the system learns from the team’s prior investigations, enabling it to \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"automatically classify and prioritize new failures\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\". This means engineers can concentrate on genuinely new or critical issues, while known or recurring problems are automatically flagged and categorized. Additionally, the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"bulk analysis\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" feature helps streamline this workflow by grouping similar failures together, so teams can resolve multiple instances of the same root cause in a single action rather than reviewing each occurrence individually.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Additionally, with ReportPortal’s widget-driven dashboards, you can design a reporting layout that continually refreshes as new results are posted. Instead of repeatedly generating manual test reports and summaries, you establish a set of \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/ReportingAndMetricsInReportPortal/#test-automation-reports-in-reportportal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"key metrics\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" and visualizations upfront. From that point forward, the dashboard updates in real time, ensuring that everyone has instant access to the most current testing insights without any extra overhead.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Lastly, measure your \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"code coverage\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", where \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://drill4j.github.io/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Drill4J\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" will be a great help to you. It provides metrics backed by hard data and instant feedback on testing progress. Drill4J is integrated across multitude of environments and testing stages, including unit, API, E2E, and manual tests.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"A unique feature of Drill4J is its ability to aggregate data from \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"multiple application versions\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", enabling the incorporation of weeks of testing metrics into a cohesive and actionable report.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5c3lfgktOZodMOMa9bB7nG\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Drill4J supports short-term analyses for cases like branch merging and pre-release testing with “New or Changed Methods” coverage feature as well as detailed all-encompassing report for long-term test development planning.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"2vE3bsC04WcdADAxsyFlb8\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Test automation offers numerous benefits, but it requires good planning and management. By calculating ROI, you can ensure that automation is a smart investment. Avoiding common mistakes and focusing on clear goals will help you get the most value from automation. By the way, ReportPortal boosts ROI by cutting down manual work, providing useful insights and supporting continuous improvements.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Test automation ROI"},"leadParagraph":{"leadParagraph":"Test automation is now a key part of successful software development, helping companies release better products more quickly. While manual testers still play a crucial role, automation testing offers faster results, increased test coverage, and savings in manual test execution time."},"category":["Other"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4OsXLKIt5gLXAkDFkUe5DD/d8479f0b8d97f4bfcefe59812d6edd6f/ROI-icon.png"}},"slug":"test-automation-roi"}]}}}