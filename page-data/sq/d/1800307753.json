{"data":{"allContentfulBlogPost":{"nodes":[{"id":"197bd3c7-8718-5202-b4b4-e7f87bbd711d","date":"November 21st, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky tests are the wild cards in the software testing world. Flaky tests happen when a test often changes its status from passed to failed and vice versa under the same conditions. Flaky tests can cause a lot of delays, questions, and chaos for software developers and QA teams.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky test cases table widget \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal allows to find flaky tests in your runs. For that, create a Flaky test cases table widget in our test automation results dashboard: specify the Launch name and the Launches count for comparison.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"blueBg The widget is built by the name of the launch, and not by the filter. To obtain all the data in this widget, make sure that the launches in which you want to address flaky tests have the same name.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Flaky test cases table widget shows the 50 most unstable test cases in the selected launches. This includes not only Failed tests but also tests that change their status from Passed to Failed, from Failed to Passed. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This widget is dynamic, it only needs to be built once, and it will update automatically.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6wnzPPQQjr0Mj5OFxl3u9G\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"When creating the widget, we can also specify whether to include Before and After methods. Before and After methods are preconditions/postconditions, for example, creating test data, and then, after running the tests, cleaning them up. The reason for flakiness is not always within the test itself – the methods can also be flaky. For instance, the test itself might fail if generating test data fails. In the Launch, the reporting may be fine, and the test case would have passed if the Before or After method was functioning correctly.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Therefore, when a certain number of runs have accumulated, especially in the case of large launches, it is challenging to analyze failed automated tests. You can create this widget, investigate the unstable tests, and understand if there is an issue with the Before/After methods. The problem may not be in functionality, but in automation. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Additionally, with the flaky test widget, you can identify issues with the environment or with a specific version or branch. For example, if a test runs on different environments and passes in some, fails in others, you can temporarily remove these tests from the scope until the environment-related issue is resolved, saving time on running and analyzing flaky tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"How to avoid flaky tests?\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\nFor those seeking ways to prevent flaky tests, consider the following best practices:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Stable Test Environment\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Check the availability of critical resources for test execution. Control external factors (e.g., system resources, network connectivity).\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Test Isolation\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Decrease the number of connections between tests. It can significantly minimize the risk of instability.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Optimal Test Timing\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Properly plan your tests to ensure optimal performance for your testing environment. Remember about network congestion and system load.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Test Data Management\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Do not use mutable or shared data that can cause flakiness.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Overall, if you know what causes flaky tests and apply best practices to prevent them, you can reduce the impact of flaky tests. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Flaky test: best prevention practices"},"leadParagraph":{"leadParagraph":"Flaky tests are the wild cards in the software testing world. Flaky tests happen when a test often changes its status from passed to failed and vice versa under the same conditions."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4eQDfyCFkL2RPwYbGorwjN/f8c1371ff85238ec12c9636e7ce82110/Flaky.svg"}},"slug":"flaky-test-best-prevention-practices"},{"id":"37584917-9729-5501-af19-4f5118a55c69","date":"October 27th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Nowadays, test automation is an essential part of the software development process. For effective test automation, it is crucial to build a QA metrics dashboard. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"But firstly, we have to complete some prerequisites.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\rPreconditions\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"1. Build a testing pyramid for the project. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Specify the number of tests at each level that are planned to be automated and those that are already automated to understand the coverage.\\n\\nFor example, we can have the following suggested groups: A, B, C.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A - the most critical tests that we would like to automate with 80% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"B - we work on these tests as a second priority, aiming for 60% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"C - we expect 20% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"unordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It is important to appropriately mark test cases in the project management tool (for example, Jira, Rally, Gitlab, Trello) - what is planned to be automated, and what is already ‘In progress’. \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"2. Add attributes to test cases. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can use attributes to specify the area to which the test cases belong to, for example, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'component: UI'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". Additionally, attributes can indicate the scope of these tests, such as \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'label: smoke'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". In the attributes you can also specify the environment where the tests are run, as well as the branch, plugins, and third-party systems. When the same scope of test cases is executed against different environments, versions, or with different plugins, it is beneficial to indicate these differences in the attributes.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6JxigKYbkzoH5suIMsSThF\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, we have completed the preconditions, and now we are ready to create the test automation results dashboard. We recommend creating different dashboards for various purposes: for regression testing, for specific versions, for unstable tests, etc.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1lolQnY68IuB2PDbouJOVC\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In ReportPortal, you can track test automation metrics using our widgets.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Widgets for tracking test automation\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Component Health Check\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can create this widget using attributes and see which components, functionality, platforms, etc. are not working correctly so you should pay more attention to them.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3mFDnowm9naXOLAYOh8rPE\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Overall Statistics\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This is a summary of test cases with their statuses. In the test result report, you can only set Total/Passed/Failed/Skipped. Additionally, this widget shows the number of bugs categorized by defect type: Product bugs, Automation bugs, and System issues or custom defect types. The widget has clickable sections, allowing you to navigate and view test failures and their causes. Afterward, post an issue in the bug tracking system (BTS) for all Product and Automation bugs or link the already created tickets to the failed items in ReportPortal.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4ni9dNIqFOWdhuWUv3x5EK\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\rUnique Bugs Table \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This widget displays existing bugs created in the BTS. It enables us to track the issues within our product. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2ybWGW8dyXbLEid1nzACpI\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Passing Rate Summary \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Initially, it is sufficient to understand the proportion of Passed/Failed cases.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Here, you need to pay attention to the failures: are there any tests that are constantly failing and that don’t yet have a defect type?\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4egJWwjKdLnoh1FZXi3m40\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky test cases table \\r  \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The widget displays the most unstable tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It's not just a failure when we know, for example, that it's an automation bug. But if a test first passed, then failed, and then passed again, it means the issue is either in the test itself or in the environment. We need to investigate everything we see here.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3uHxxD6GW09TA6jIL1kshz\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Cumulative trend chart\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You only need to build it once to obtain launch summary statistics with one attribute. For example, if we have an \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'environment: dev'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" attribute, it's convenient to view statistics for this environment.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In the ReportPortal, there's a peculiar feature: if a launch has a certain attribute, and there are no attributes at the step level, then all tests within that launch are associated with that attribute.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Therefore, if we have an \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'environment: dev'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" attribute, the system treats all tests within that launch as if they also have this attribute. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4E7UzDHdqkjzOBI9U7Xvmf\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In summary, test automation metrics help to enhance test automation, and as a result, contribute to the creation of high-quality software.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"How to create a QA metrics dashboard in ReportPortal"},"leadParagraph":{"leadParagraph":"Nowadays, test automation is an essential part of the software development process. For effective test automation, it is crucial to build a QA metrics dashboard. \n\nBut firstly, we have to complete some prerequisites."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4BXb5XyjHaWoKZDWt78h6n/42688cc418cd33231ce60a5ff154ce6f/metrics.svg"}},"slug":"how-to-create-a-qa-metrics-dashboard-in-reportportal"},{"id":"ee314060-16fa-5339-ab90-d468224f1d4e","date":"October 20th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Software testing can be a bit of a joke sometimes! You never know what’s going to happen! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We have collected comical stories to highlight the humorous side of the serious world of software testing. Let’s read! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Email Extravaganza \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On one project, automation test report was supposed to arrive in the inbox when test results came back unsatisfactory. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The team tested the functionality, and everything went smoothly. A couple of years later, new QAs joined the team. They decided to revisit the old test cases, maybe thinking the email server was on vacation. Testers began generating fake reports. And guess what? The client started getting emails with weird text! By evening, the client's inbox was overflowing with email mayhem! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It turns out that someone had accidentally hardcoded the customer's email address and forgotten about it. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, the crisis averted, and lessons learned! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1rJGd7Z5bqgNEOHaGmki5G\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Emoji Surprise \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On a typical day, testers asked their colleague for a link to the smoke test results.  \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The tester sent the link. But then, out of nowhere, some URL-ID keys turned into smoking emojis! Everyone tried to click the link with bated breath. But the link was broken, with emojis hidden inside. The team laughed. Our clever tester said, “As you see, it's a 'Smoke' test!” \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4JixwuKz2UcHbQa2CURP0h\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Unintentional Spam \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On the project where a telehealth system was being built, the testers needed to test out things like user creation, appointment scheduling, and so on. They used random phone numbers and email addresses. But guess what? When they connected it to the SMS gateway, the “random” phone numbers turned out to be real, and actual people started getting SMS notifications about appointments with a venereologist! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"These unsuspecting individuals thought they were getting spam and started complaining. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The client company couldn't believe what was happening, so they urgently bought SIM cards for test purposes. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"79pGknQ2dIlXMqWt8rZFr6\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The “Phantom Mouse” \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"One day, a QA team was examining a video editing software that allowed users to edit, enhance, and export videos. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A tester encountered a confusing situation: the cursor glided across the screen, pressing buttons without any input. Could it be a ghost? But fear not! It was just a wireless mouse with low battery, sending out random signals, causing chaos in the video software. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Sometimes the solution is simpler than we imagine! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"53NCcaS2JUdFncFettu8O7\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Time-Traveling Timestamp \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"While testing a messaging app, a tester noticed that one of the timestamps showed a message sent from the future. Shocking! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This caused a great deal of confusion among the developers, but they couldn't replicate this issue. The tester tried various combinations but eventually figured out that the date and time on the phone were set incorrectly. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A simple correction, and voila! Back to the present, my friends. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1ad0p9elvcKrjEpjqus8tS\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"These stories serve as a reminder that even in the serious world of software testing, there's always something to laugh about. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Funny situations in testing"},"leadParagraph":{"leadParagraph":"Software testing can be a bit of a joke sometimes! You never know what’s going to happen! \n\nWe have collected comical stories to highlight the humorous side of the serious world of software testing. Let’s read! "},"category":["Other"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/5txL6K9hal4rlrRHCCueI9/37e81c2db1753c52cd201370038cea3c/Funny.svg"}},"slug":"funny-situations-in-testing"}]}}}