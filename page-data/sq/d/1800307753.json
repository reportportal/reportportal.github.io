{"data":{"allContentfulBlogPost":{"nodes":[{"id":"483d07b1-609d-5fb1-99c4-80414f5ad058","date":"May 16th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Choosing the right test reporting tool is an essential step for any organization looking to optimize their testing processes and enhance software quality. Your choice should be based on multiple factors such as ease of use, integration capabilities, customizability, and of course, the most important factor – a comprehensive set of features.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal offers real-time reporting, data visualization, integration with different test frameworks and bug tracking systems (BTS), compatibility with CI/CD pipelines, and some intelligent features for smarter and faster test results analysis. One of these features is Auto-Analysis. Today, we will share tips on how to use it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Analyze the tests that failed in the first run\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To operate the Auto-Analysis, manually analyze the test results of the first run. Afterwards, based on these data, Auto-Analysis will automatically assign defect types in subsequent runs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/SearchForTheSimilarToInvestigateItems\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Search for the similar \\\"To investigate\\\" items\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" and \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/UniqueErrorAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Unique Error Analysis\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" features will help you to analyze the results faster.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg If you inaccurately identified the defect type when analyzing the results of the first run leading to Auto-Analysis consistently labeling an issue as a Product Bug, for example, when it is actually an Automation Bug, you can manually adjust the defect type to Automation Bug through the \\\"Make Decision\\\" modal. During the subsequent run, Auto-Analysis will then label it as an Automation Bug.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Use Analyzer settings\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Adjust all necessary components within the Analyzer settings, ensuring it suits your needs and enhances its usability.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Firstly, specify which launch you wish to use as a base for Auto-Analysis.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1HmDdMOJlaMesdpXkoCP5T\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Following that, indicate the minimum percentage match between the log being analyzed and other logs being compared with it. If you require maximum accuracy, set a higher percentage. However, to understand how Auto-Analysis generally operates, you can lessen percentage value.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The default setting stands at 95%. This is the recommended similarity to prevent any future confusion regarding what the Analyzer may have misinterpreted, and to maximize confidence that everything has been correctly identified.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6NTwbhFsZMBdV5iK4bmeI\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Next, determine the number of log lines to consider. Should it encompass the entire log, or merely a few lines, particularly if the root cause of your test issues is concentrated in the initial log lines followed by some stack trace?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4ItDDG9bOU6ohyPEXeHxme\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Following this, consider whether you want Auto-Analysis to involve smaller logs. When the checkbox \\\"All logs with three or more rows should match\\\" is checked, Auto Analysis will consider only the first three log lines to compare logs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6lBZx9ToNdkc3PqI5OhUCA\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Use Auto-Analysis with \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Make Decision\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" modal\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can also make the decision on your own – in \\\"Make decision\\\" modal, select the necessary defect type for the failure. \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/MLSuggestions\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ML Suggestions\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" functionality can assist you.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"24pmuDxKpgGi2qIJ2vDVVr\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4nkgHzXPJfOBFCFTtACAx1\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"4. Track the history of changes in the \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"History of Actions\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This function allows visibility of all actions taken with the test item, such as when and what type of defect was assigned and based on which item.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"41EBkQXNPCQKwXExB958Ih\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"5. Use BTS along with Auto-Analysis\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If you created a bug in BTS and linked it to the test item, Auto-Analysis will not only assign a defect type for the failed test in future runs but will also attach a link to the corresponding issue in BTS. This is highly convenient as it allows you to view the bug and its status instantly.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6tIxddoOpFLcrfRruRNLEo\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Thus, the Auto-Analysis feature uses AI algorithms to analyze test results automatically. It allows to focus more on testing strategies and improvements rather than spending time on manual analysis. With this innovative feature, you can speed up testing processes, improve accuracy, and ultimately deliver high-quality software products.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can find more information about test automation results analysis in our \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/category/analysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"documentation\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Auto-Analysis feature: best practices"},"leadParagraph":{"leadParagraph":"Choosing the right test reporting tool is an essential step for any organization looking to optimize their testing processes and enhance software quality."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/1Qxw1b8hjTJHYMzv8bWfI4/8b3a41a2dc1f9e2219ea7ca2cc1af054/Property_1_Variant16.png"}},"slug":"auto-analysis-feature-best-practices"},{"id":"c6ee6761-6b6f-5a38-81a8-4d55a20c2be9","date":"April 9th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Defect triage or bug triage is a process of reviewing, categorizing, and prioritizing the reported bugs. The failure categorization plays an important role in the defect triage process as it helps to determine the type of failure. With ReportPortal, you can easily group the defects into categories.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Best practices of defect triage process include bug detection and categorization, bug documentation in the bug tracking system (BTS) and regular bug triage meetings. Let’s look at each of these points.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to triage defects with ReportPortal\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Using ReportPortal, you can easily carry out categorization of failures based on issue roots.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To do this, open the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/ManualAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Make Decision\\\" modal\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" for the \\\"To Investigate\\\" item and review logs, screenshots, and video recordings. Based on this comprehensive analysis, you can categorize this failed item. In ReportPortal there are 4 main defect types groups: Product Bug, Automation Bug, System Issue, No Defect. There is also the option to \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/reportportal-configuration/ProjectConfiguration#custom-defect-types\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"create subcategories\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In addition, during manual defect triage, you can take advantage of our ML-powered failure reason detection. If you’re not sure which category to put the bug in, use a hint from \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/MLSuggestions\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ML Suggestions\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". Also, pay attention to the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/SearchForTheSimilarToInvestigateItems\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"similar \\\"To Investigate\\\" items section\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", which displays test items with similar logs. In this way, the defect type can be applied to many test items at once.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Moreover, ReportPortal automatically groups tests by the same \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/UniqueErrorAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"unique errors\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", which also speeds up test failure analysis.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to create defect in BTS\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"First, the team needs to determine which fields are required. Typically, when creating a bug (or defect/issue, which are the same thing), the following fields are filled in: summary, steps to reproduce, estimate, severity, priority, and attachments.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It is also recommended to link the bug to other work items, like user stories or requirements, so it is not isolated. This way, you can more efficiently pinpoint the root of the problem, determine when it occurred, and identify potential causes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What is the bug triage meeting in the scrum process?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This is a session where bugs are reviewed and discussed, typically with the participation of customers, developers, and testers. During a bug triage meeting, each bug is analyzed in detail, and its severity and priority are determined. If it is clear who will handle the bug, it might be assigned to that person during this meeting. If it's not, the bug is assigned to either the Backend or UI team.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"As a result, there are no uncategorized bugs left in the backlog. This greatly helps in the future, allowing for a swift understanding of what needs to be fixed first and what can be addressed later.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What's the difference between severity and priority? The bug's priority is assessed based on its importance for a specific version. For instance, during a bug triage meeting, discussion can be held about when this bug fix should be ready. If there's a bug that the customer wants to be fixed as quickly as possible and in the most recent version, it will be given the highest priority. On the other hand, severity is a measure of how critical a bug is to the application's operation. For example, if a login button looks different from the design, it doesn't crash the application, so the severity of such a bug would be low. However, its priority would be high because it's visible on the homepage and presents an aesthetic issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Regular bug triage meetings can help avoid scenarios where only the reporters are aware of the identified bugs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What are the benefits of the defect triage process in agile?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Effective workload planning in the team.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The tasks can be distributed based on what needs to be fixed first and what can wait.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Risk Management.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Through the triage process, defects are evaluated according to their severity and the impact they have on the software. This allows for the reduction of the likelihood of serious issues.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Improved Communication.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If a stakeholder is present at a bug triage meeting, the expected results for bugs with unclear requirements can be discussed immediately to prevent the team from having to redo everything multiple times later.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"After defect triage, testers can build a \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/blog/how-to-create-a-qa-metrics-dashboard-in-reportportal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"QA metrics dashboard\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". This dashboard provides a visual representation of all bug-related statistics: the total number, how many Product Bugs/Automation Bugs/System Issues there are, how many bugs are in a specific version, and the project's most problematic area.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In conclusion, ReportPortal can significantly facilitate defect triage by enabling comprehensive analysis and categorization of failures. Defect/Bug triage is a crucial step in software testing. By systematically categorizing and prioritizing reported bugs, teams can streamline workflows, enhance communication, and ultimately contribute to the successful delivery of high-quality software products. \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Defect triage with ReportPortal"},"leadParagraph":{"leadParagraph":"Defect triage or bug triage is a process of reviewing, categorizing, and prioritizing the reported bugs. The failure categorization plays an important role in the defect triage process as it helps to determine the type of failure. With ReportPortal, you can easily group the defects into categories."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/2AhQ8yZzrVvjvn4iSigs0I/5adfbf1a35023dd1f7d983d4becc9d70/DefectTriage-icon.png"}},"slug":"defect-triage-with-reportportal"},{"id":"bf3b04fc-485a-57d9-9417-491c33675fe0","date":"March 22nd, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Typically, a test automation engineer uses the main metric of test coverage, which shows how many tests are written. However, this metric does not consider the quality of the tests. ReportPortal will assist you with this. With our help, you can build a test automation metrics dashboard to achieve a balance between speed, reliability, and relevance in automated testing. This dashboard is specifically focused on automation metrics, not quality metrics like the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/blog/how-to-create-a-qa-metrics-dashboard-in-reportportal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"QA metrics dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In ReportPortal, you can create two versions of a test automation metrics dashboard. The first one is Launch level, when we focus directly on a certain set of tests or on a specific execution. For it, we build a history and collect metrics. We gather information for launches, showing information on how a particular Launch is progressing, and all the metrics above it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The second version of the test automation metrics dashboard involves aggregated widgets. Essentially, this dashboard centers around a unifying object: a build, a sprint, or a release.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rLaunch level dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rInvestigated percentage of launches\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows the number of items in launches with a defect type “To Investigate” and corresponding subtypes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If there is a tendency not to deal with failed autotests, it means they are not useful.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Either automation engineers understand in advance that these errors are not serious, and it is not necessary explore “To Investigate”. Then the question arises, why these autotests were written, which do not make significant checks. After all, autotests should be of maximum benefit, all failures should be dealt with to indicate some kind of errors.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Therefore, with this widget, we can be confident that the team has reviewed and categorized all failures.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3fGMvrb1NC1E55K6tRufDi\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rLaunch statistics chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAs soon as this categorization appears, we get a new level of information – a distribution of failure reasons. We can simply look at the number of Failed and Passed tests. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"70NVNN3fiBLdXNCce6Jbz0\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rOr we can use ReportPortal's capabilities for categorizing failure reasons. This way we can understand the reasons why the tests didn't pass, and how many tests relate to each failure reason category.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"There are three main groups of failures: Product Bugs, Automation Bugs, and System Issues.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"System Issues. This is a category of problems related to technical debt of the infrastructure. One could say it is a DevOps debt, that is, the debt of the engineering team in setting up the infrastructure for flawless automation work.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What relates to the Automation Group – this is the technical debt of the automation team to fix all the problems, make automation stable, make locators stable, perhaps, update test suites.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Product Bugs with all its subcategories are those very test cases that help make the product better.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"NGAV3P6K3FN7Dg3Y2xZdb\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rFlaky test cases table (TOP-50)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows 50 unstable tests in the specified launches. When setting up, you can specify whether to consider before and after methods.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget helps to identify unstable autotests that pass checks every time with a different status. Next, we need to understand: maybe it’s not the product itself, but the autotest? For example, there is no response from the server, and the test fails because there was no update on request.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"One flaky test may not be a big problem, but several of them can certainly spoil the process. For example, if we then want to build Quality Gates that trigger automatic deployment after a health check, flaky test cases will hinder this capability.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It is recommended to fix the flakiest tests first, or even exclude them from the general run for some time.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"35jGrv2gqnIfAfC1dW2kRZ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Launches duration chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget shows the duration of the selected launches.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It’s best to build a filter for this widget based on launches of a certain direction, for example, regression, and against one environment. Then we can assess how automation is working and whether there is a scope of tests that stand out from the overall picture. Some launches may take too much time. For example, in the picture below, one launch lasted almost an hour. Next, we can look at the autotests in the problematic launch: maybe there is a test that takes a long time and the whole run takes a lot of time because of it. Perhaps it will be faster to carry out this check manually? Or, perhaps, it would be better to divide this large scope into several to speed up? Sometimes there is simply a lot of everything in one scope, and this is not always justified.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Some projects initiate extensive runs over the weekends, but good automation should still prioritize relative speed.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"AJrrcyzeCoqgCTxv0U4sr\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rTest-cases growth trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis chart shows how many new test cases have been written in each run. It allows observing if the team generally adds or excludes something from one run to the next.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4PtoF1YRSiHclhkJdzIWEn\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rMost failed test-cases table (TOP-50)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAnother important metric. If these tests fail for reasons other than Product Bugs, it means they are not testing the application, but are failing for some other reasons. It's necessary to verify why they failed and fix it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4YCZGgJyrsf0raHjFtM6Sf\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failed cases trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget will allow to understand whether the team is moving towards reducing the causes of failure or increasing them.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"74a3nDOqrCmmRsZ9TzQwxb\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rMost time-consuming test cases widget (TOP-20)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows test cases that take the longest time in the last run of the specified launch.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It's convenient to use it in combination with Launches duration chart: as soon as we see that the launch takes a long time, we build Most time-consuming test cases widget by the name of this launch and look which test cases take the longest.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In the example below, the longest test case takes almost 8 minutes. We should think about whether we can somehow reduce it. Or, if this test always passes, we can think about how relevant it is and whether such a check is needed. Perhaps this is already a stable feature, and the autotest no longer has much value, and then we can remove it from the scope and save 8 minutes on the run. And if the test takes a long time, and periodically fails, then we need to figure out what's wrong with it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5PQIrXStlSmpl5I7faERyE\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAggregated level dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rCumulative trend chart \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget allows you to collect all launches related to a particular build, release, or version into a general data array and show how many Passed/Failed tests there are and the reasons for their failures.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, against the nightly build there were automatic jobs run that were related to API tests, integration tests, and end-to-end tests. We're interested in looking at the overall summary of the build, and how it passes. Not looking at individual jobs but looking at the summary of all results.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget can collect values by some attribute and show them over time. Accordingly, this way we can see how the stability of tests is improving. In the screenshot below, you can observe that initially most tests failed. The team started to add new test cases and stabilize the tests. At the second stage, you can see the emergence of System and Automation issues. Then, System Issues were fixed, remaining are Automation and Product Bugs, and so on until all failures decreased to a minimum value.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"zJoyno23jC1fYsvqPpPjY\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rOverall statistics\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThe aggregated mode of this widget also can collect a summary of all launches that can relate to a build, a team, or any other object that we group.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Separately, you can look at the distribution of failure reasons.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6MF74Ay2gUiJ48oboFpAdv\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rTable Component health check\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget allows you to view the state of the components we have tested. It can also look at the results of a specific build or selection.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget takes all the launches that fall into this selection, combines them into one common set of data, and subsequently can slice them by the attributes of the test cases that are in these launches.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In this case, at the first level are the features that are being tested. At the second level – split everything by priorities. At the third level – split by operating systems. At the fourth level – split by scenarios. Here you can add up to 10 levels. Including, you can add a custom column to see, for example, where the tests were run.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In this case, the features filtering, statistics, permissions, re-tries were tested. According to the settings, the Passing rate should be at least 70%, so the permissions component got a Failed status: out of 5 tests 3 tests failed, and they all relate to Product Bugs. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4sOpwUf6e9XvEOBgGTp3k7\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rIf we go deeper, we see that our tests are failing for critical functionality, medium and minor. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"43Y36MGom9aibIQgzp7CZY\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rWe go deeper, we see a distribution between Android and iOS. We go deeper and see that the user assign scenario is failing. We can go further and look at the selection of failed tests.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5oY5rXx71kjOawFsxFXvSh\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget gives us the possibility of granular viewing of all components and types of tests that were run. You can closely examine each specific component or critical functionality that has been tested. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Project activity panel\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget shows what was done on the project.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget is useful for the understanding who is working on what. Sometimes you go to a launch, and the defect types have already been set, some items are missing. You can go to the Project activity panel and see who worked with this launch.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Also, when building a widget, you can specify a user name and see the actions of someone specifically. For example, it is visible when the user launched the launch, when he finished it, when he did the import, when he created widgets/dashboards, or some project integrations, linked an issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1vfV1HOZy6vRfbl6wR5Nqn\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Regular review and improvement of test automation metrics help identify areas that need enhancement. This process streamlines test automation workflows and provides insights for decision-making. Thanks to these metrics, organizations can deliver high-quality software products and remain competitive in the market. \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"How to create a test automation metrics dashboard in ReportPortal"},"leadParagraph":{"leadParagraph":"Typically, a test automation engineer uses the main metric of test coverage, which shows how many tests are written. However, this metric does not consider the quality of the tests. ReportPortal will assist you with this."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/3vgMLnroiqlSxLyorXOpnM/832dda2f718570fa97a41793be29ece9/TestAutomationMetrics-icon.svg"}},"slug":"how-to-create-a-test-automation-metrics-dashboard-in-reportportal"}]}}}