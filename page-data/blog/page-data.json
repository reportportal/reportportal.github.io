{"componentChunkName":"component---src-pages-blog-tsx","path":"/blog","result":{"data":{"allContentfulBlogPost":{"nodes":[{"id":"95cf44a3-d185-589e-b45c-b723721aa95b","slug":"reportportal-integration-with-playwright","date":"June 6th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"If the Playwright used on your project to write and run tests, the results can reach the ReportPortal via the agent-js-playwright. The agent is a custom Playwright reporter that sends test execution results to ReportPortal.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Let's walk through the steps to set up integration.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can also watch our \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://www.youtube.com/watch?v=UdeRKqFVcuU\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"video guide\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" about ReportPortal integration with Playwright.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Installing the Agent\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The first step involves installing the agent into the project. There is a small project with a few Playwright tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1nTAlGJqbjfdnS7h6GyHy0\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The tests are simple.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"709LevVIPcekCCDwl0JeVH\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The agent package is available on \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://www.npmjs.com/package/@reportportal/agent-js-playwright\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"NPM\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" and \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/reportportal/agent-js-playwright/pkgs/npm/agent-js-playwright\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"GitHub\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" registries.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Below, you can find the commands to install it using npm, yarn or pnpm.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3pcVubb9L0wmDD2ViUfduW\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After installation, it will appear in the list of dependencies. At the time of writing this article, the latest version is 5.1.8.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After the installation, it is required to provide some ReportPortal configuration data.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Configuration of the Agent\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The next step involves configuring the agent and providing some basic options that will connect ReportPortal with the project.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"First, create a Playwright config file if you don't have one.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The basic configuration is presented below.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6NBEzYmQrTWhxVhz3lOd8R\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The ‘dotenv/config’ used to get access to the ENVIRONMENT variables specified in ‘.env’ file. \\nIt will be used to store the API key value.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"To obtain the configuration options, the ReportPortal profile page can be used.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"1. Open the ReportPortal instance (if you don’t have one the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://demo.reportportal.io/\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"demonstration instance\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" can be used) and log in. For a demo instance the GitHub login can be used.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6YEiKr4JyoNApF7JrhzbiV\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"2. Then navigate to the profile page. There, we can find a configuration example for Node.js.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2ko0TPPf2mMUcQtoXriC9t\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"19ZDr5Ymwxgkl7GybPNaRq\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"3. Simply copy the JSON object and paste it into your project file assigning it to a variable, f.e. ‘rpConfig’.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Let’s name the launch as f.e. ‘Custоm regressiоn with Playwright’ right after that.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"nCovLjtPRmJuOPH2IuH4B\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"4. Provide an API key which can be generated on the API keys page in your profile.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Open the API Keys page and click on ‘Generate API Key’ button.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"7i8SAdq5mST6hCC5qxbWMR\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Specify the API Key name and click on ‘Generate’.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1MsddJp7f2jbXbPsX1PAHK\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Detailed API Keys description can be found in the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/reportportal-configuration/HowToGetAnAccessTokenInReportPortal#2-authorization-with-users-api-key-for-agents\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"documentation.\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After generation the API Key can be copied to use it in the code.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Paste it as a value for \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"REPORTPORTAL_API_KEY \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\"in ‘.env’ file and update ‘rpConfig’.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1LZQxK56uqvo8yEEJrX6g1\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"5. Instruct Playwright to use the agent as a reporter.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"CJhE4gwDETX84jXX0f0LK\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"NOTE:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The full list of options with their description can be found in the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/reportportal/agent-js-playwright\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"agent’s repository\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\".\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"blockquote\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Running the Tests\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After completing the two steps above, run tests as usual.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"1. Run \\\"npm run test\\\" command.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"53rF6MIMUyj0PKQ4tI3dSF\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"2. Monitor the test's progress in ReportPortal. The state can be refreshed to obtain the latest results.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"z8KLSnztBrPCBjJ5nUJvD\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Let’s check the embedded reporter of Playwright. The statistics here match exactly.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"v3obYglLLhKqrUlrVoVDb\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Update tests with additional data\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can enrich your Playwright test report by adding additional data. This includes description, attributes, logs, attachments, TestCaseID and status.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Description and attributes can be used to specify any extra information about the test and can also be used for filtering and building \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/WidgetCreation\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"some widgets\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\", like \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/ComponentHealthCheck\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"\\\"Component Health Check\\\"\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\". Extra logs can also be provided together with attachments. The TestCaseID can be used as a \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/work-with-reports/HistoryOfLaunches/\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"history ID\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" across different launches. There's also the option to change the status in ReportPortal for a particular test result.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"How to add additional data to the tests?\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"1. To provide \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"attributes \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\"and a \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"description\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\", import the \\\"ReportingApi\\\" from the agent package and use it within tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"33Md1YWI5AJygylmNgyRhi\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3zFfE87dcB2iVFsKOa3ShA\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After running the tests again, the specified data will be visible in ReportPortal in the newly appeared launch. We just need to drill down into it to see the results.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"Q1xrxhzclFddI0olgVMw6\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2vamLOlNHryMSMe8uBC0of\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"2. To add \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"logs\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" with specific log levels, use the Reporting API methods or report as is if you're writing logs to the standard output.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2XLXcJFZjLhA1wooM6Qost\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Let's run our tests again and check the new launch in the system. To see the logs for particular test result, we can drill down to its latest level.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"48D0nAVyNmuCcnHleLxBzZ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The error logs thrown by the failed tests are reported by default on the test end.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6kwmo3Bf7aYyzGs1i83UTj\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"3. To insert \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"attachments\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\", call the \\\"attach\\\" method on the \\\"testInfo\\\" object provided by Playwright.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6inGRpaZZJNxM1KkQjjV7o\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After launch appearing in ReportPortal, the attachments can be found on the same place as logs.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"36uQ6YAb1OZXkcBDBbJf9Q\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"4. The \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"TestCaseID \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\"is a unique test case identifier across all launches in ReportPortal. It is generated automatically based on code reference (physical location of the test within the project) and parameters (if exists). But the real test case ID from your test case management system can be used instead. To provide TestCaseID explicitly, call the \\\"setTestCaseId\\\" method of \\\" ReportingApi\\\" and provide a value.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"7AOw7xpXskhvPUedwXiMKu\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Applied TestCaseID can be checked via \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Test item details\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" modal window.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1S41AJdDQwp4Vods9BVLvg\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"NOTE:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Once the TestCaseID changed for the test result, it will be unlinked from its previous executions from historical data perspective.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"blockquote\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2zS5nkm7XiCp2QKXsVPRvR\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"History of this test result within previous launches, where the TestCaseID was persistent between runs.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1Rmgac68ilOhQzYGKXEwlJ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"5. In case for some reason necessary to set the specific \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"status\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" for test result in ReportPortal only, the \\\"ReportingApi\\\" \\\"setStatus\\\" methods can be used.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2IPxqsztmNFilcUqtd8h9H\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Now in ReportPortal we can check that test that actually failed marked as ‘passed’ in the system.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"44FF632JrDsUtUQIQmjoRm\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"NOTE:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Detailed information about the additional data and methods for adding it can be found in the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/reportportal/agent-js-playwright\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"agent's repository\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" within the Readme instructions.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"blockquote\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"By this step-by-step process, you can fully integrate your Playwright automation with ReportPortal. Once set up, you can view test results in real-time, which is particularly useful for lengthy and extensive test suites. Moreover, ReportPortal can analyze your data using Machine Learning algorithms. If you're looking to expedite the process, you can delegate it to the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/AutoAnalysisOfLaunches\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"ReportPortal Analyzer\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" to quickly identify the root causes of defects. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal also provides a comprehensive visualization of your automation process, allowing you to generate charts, \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/blog/how-to-create-a-test-automation-metrics-dashboard-in-reportportal\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"track test automation metrics\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\", and keep an eye on your \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/ReportingAndMetricsInReportPortal/\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"project's stability\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" with our test reporting dashboard. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Bring your Playwright test reporting to the next level! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Useful links: \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/reportportal/examples-js/tree/main/example-playwright\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Example from the article\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/reportportal/agent-js-playwright?tab=readme-ov-file#reportportalagent-js-playwright\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Agent documentation\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/log-data-in-reportportal/test-framework-integration/\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Other available integrations\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"unordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Reportportal integration with Playwright"},"leadParagraph":{"leadParagraph":"If the Playwright used on your project to write and run tests, the results can reach the ReportPortal via the agent-js-playwright. The agent is a custom Playwright reporter that sends test execution results to ReportPortal."},"category":["Integrations"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4cnsh2rGNxXIKzVzqwlwxz/db63d978313e6ec5fcd4422084a87249/Property_1_Playwright.png"},"description":"Upgrade your Playwright testing process with our test automation results dashboard."}},{"id":"7b069ade-c7d2-5402-a5d6-40c465cd6eb7","slug":"top-tips-for-using-quality-gates-in-reportportal","date":"May 30th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Nowadays, Quality Gates is a significant stage in the project delivery cycle. These checkpoints play a vital role in the development of high-quality software.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What is Quality Gate?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This is a set of predefined criteria that must be met before a software product can progress through the development lifecycle.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal offers \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Quality Gates\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" as a premium feature that accelerates the CI/CD pipeline by providing quick and continuous feedback to your CI/CD tool. There are five rules to quickly assess the build quality and understand if it can be deployed to a specific environment.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-tests-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of tests in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#failure-rate-of-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failure rate of the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-issues-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of issues in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#new-failures-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"New failures in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#new-errors-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"New errors in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Below, you will find some tips to get the most from Quality Gates.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Create separate Quality Gates for different launches\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, your project may have various automated tests such as Regression Suite, Smoke Tests, and UI Tests. And we can create different rules for these types of tests.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"27o8ih1R6oJRiNQ5dVgOeH\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We also need to determine under what conditions the Quality Gate will be considered as Passed. It's recommended that these aspects should be discussed among several interested team members to define quality criteria more accurately. For instance, a team might decide that they agree with a failure rate of 1% if there are at least 2200 tests.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1gRoVbadRs3XvP6msyziYj\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Gradually increase the number of rules\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We suggest starting with the simplest rule, the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-tests-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Amount of tests in the run\\\",\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" to track that all tests are running every time. Understand how it all operates before adding a new rule.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"77E8kKdnDSphFnk5JwGa1o\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Further, you can enhance accuracy by selecting the \\\"Tests with attributes\\\" option. For instance, the following Quality Gate with the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-issues-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Amount of issues in the run\\\"\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" rule will be Passed for the \\\"dev\\\" environment even if there is one Product Bug with the attribute \\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"dev\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\". However, for the \\\"stage\\\" environment, the Quality Gate with such a rule will be categorized as Failed.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5Cdgaw9kfQDYVeczPMrDV1\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Keep track of the relevance of rules\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"No project is static: new test cases are created, bugs are detected, some test cases are excluded from the run, and Quality Gates should be reviewed in line with requirements.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, in the screenshot below Quality Gate failed because the number of tests in the launch has changed, but the Quality Gate rule was not updated.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5pkfsy89mVdMqnob2dgq6v\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3v23aP0PccIRJjNXR0r0TP\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"4. Make the Quality Gate a stage of the CI/CD pipeline\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"By using Quality Gates in the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/IntegrationWithCICD/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"CI/CD pipeline\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", the decision to move to the next stage can be made automatically within seconds. For instance, if the tests are successful and the Quality Gate check is Passed, the build is deployed to the environment. This flow prevents the progression of the code if it doesn't meet the necessary criteria. Thus, there is no need to manually go through and analyze the test results – if they meet the Quality Gates, the next step in CI/CD will run automatically.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3xjkSWKoANIZg0AedRMLq7\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"5. Use Quality Gates to construct test automation metrics\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Just like our widgets, Quality Gate rules can also assist in developing \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/ReportingAndMetricsInReportPortal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"metrics\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" to understand the effectiveness of automation efforts.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Pass/Fail rates\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-tests-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of issues in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#failure-rate-of-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failure rate of the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Error/failure distribution\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-issues-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of issues in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#failure-rate-of-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failure rate of the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Regression test effectiveness\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-tests-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of tests in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#new-errors-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"New errors in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#new-failures-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"New failures in the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Defect density\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#amount-of-issues-in-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Amount of issues\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" in tests with specified attributes\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Test case success rate\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityRulesConfiguration#failure-rate-of-the-run\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failure rate of the run\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" – to get test case success rate, subtract the percentage of failed tests from 100.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"By following best practices, you can automate quality control and ensure that only high-quality code progresses, reducing bugs and improving software stability for users. Overall, Quality Gates provide a strategic and robust development stage in the software delivery cycle.  \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Top tips for using Quality Gates in ReportPortal"},"leadParagraph":{"leadParagraph":"Nowadays, Quality Gates is a significant stage in the project delivery cycle. These checkpoints play a vital role in the development of high-quality software."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/5pMRYmJ3xnsXkevltwnBdf/e3d9de63450f13e47b6aa38f8b33bf88/Quality-Gates-icon.png"},"description":"Gain insights on automating quality control in our qa automation dashboard."}},{"id":"483d07b1-609d-5fb1-99c4-80414f5ad058","slug":"auto-analysis-feature-best-practices","date":"May 16th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Choosing the right test reporting tool is an essential step for any organization looking to optimize their testing processes and enhance software quality. Your choice should be based on multiple factors such as ease of use, integration capabilities, customizability, and of course, the most important factor – a comprehensive set of features.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal offers real-time reporting, data visualization, integration with different test frameworks and bug tracking systems (BTS), compatibility with CI/CD pipelines, and some intelligent features for smarter and faster test results analysis. One of these features is Auto-Analysis. Today, we will share tips on how to use it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Analyze the tests that failed in the first run\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To operate the Auto-Analysis, manually analyze the test results of the first run. Afterwards, based on these data, Auto-Analysis will automatically assign defect types in subsequent runs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/SearchForTheSimilarToInvestigateItems\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Search for the similar \\\"To investigate\\\" items\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" and \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/UniqueErrorAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Unique Error Analysis\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" features will help you to analyze the results faster.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg If you inaccurately identified the defect type when analyzing the results of the first run leading to Auto-Analysis consistently labeling an issue as a Product Bug, for example, when it is actually an Automation Bug, you can manually adjust the defect type to Automation Bug through the \\\"Make Decision\\\" modal. During the subsequent run, Auto-Analysis will then label it as an Automation Bug.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Use Analyzer settings\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Adjust all necessary components within the Analyzer settings, ensuring it suits your needs and enhances its usability.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Firstly, specify which launch you wish to use as a base for Auto-Analysis.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1HmDdMOJlaMesdpXkoCP5T\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Following that, indicate the minimum percentage match between the log being analyzed and other logs being compared with it. If you require maximum accuracy, set a higher percentage. However, to understand how Auto-Analysis generally operates, you can lessen percentage value.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The default setting stands at 95%. This is the recommended similarity to prevent any future confusion regarding what the Analyzer may have misinterpreted, and to maximize confidence that everything has been correctly identified.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6NTwbhFsZMBdV5iK4bmeI\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Next, determine the number of log lines to consider. Should it encompass the entire log, or merely a few lines, particularly if the root cause of your test issues is concentrated in the initial log lines followed by some stack trace?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4ItDDG9bOU6ohyPEXeHxme\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Following this, consider whether you want Auto-Analysis to involve smaller logs. When the checkbox \\\"All logs with three or more rows should match\\\" is checked, Auto Analysis will consider only the first three log lines to compare logs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6lBZx9ToNdkc3PqI5OhUCA\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Use Auto-Analysis with \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Make Decision\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" modal\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can also make the decision on your own – in \\\"Make decision\\\" modal, select the necessary defect type for the failure. \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/MLSuggestions\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ML Suggestions\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" functionality can assist you.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"24pmuDxKpgGi2qIJ2vDVVr\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4nkgHzXPJfOBFCFTtACAx1\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"4. Track the history of changes in the \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"History of Actions\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"\\\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This function allows visibility of all actions taken with the test item, such as when and what type of defect was assigned and based on which item.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"41EBkQXNPCQKwXExB958Ih\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"5. Use BTS along with Auto-Analysis\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If you created a bug in BTS and linked it to the test item, Auto-Analysis will not only assign a defect type for the failed test in future runs but will also attach a link to the corresponding issue in BTS. This is highly convenient as it allows you to view the bug and its status instantly.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6tIxddoOpFLcrfRruRNLEo\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Thus, the Auto-Analysis feature uses AI algorithms to analyze test results automatically. It allows to focus more on testing strategies and improvements rather than spending time on manual analysis. With this innovative feature, you can speed up testing processes, improve accuracy, and ultimately deliver high-quality software products.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can find more information about test automation results analysis in our \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/category/analysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"documentation\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Auto-Analysis feature: best practices"},"leadParagraph":{"leadParagraph":"Choosing the right test reporting tool is an essential step for any organization looking to optimize their testing processes and enhance software quality."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/1Qxw1b8hjTJHYMzv8bWfI4/8b3a41a2dc1f9e2219ea7ca2cc1af054/Property_1_Variant16.png"},"description":"Automated defect triage with Auto-Analysis feature in ReportPortal"}},{"id":"c6ee6761-6b6f-5a38-81a8-4d55a20c2be9","slug":"defect-triage-with-reportportal","date":"April 9th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Defect triage or bug triage is a process of reviewing, categorizing, and prioritizing the reported bugs. The failure categorization plays an important role in the defect triage process as it helps to determine the type of failure. With ReportPortal, you can easily group the defects into categories.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Best practices of defect triage process include bug detection and categorization, bug documentation in the bug tracking system (BTS) and regular bug triage meetings. Let’s look at each of these points.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to triage defects with ReportPortal\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Using ReportPortal, you can easily carry out categorization of failures based on issue roots.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To do this, open the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/ManualAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Make Decision\\\" modal\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" for the \\\"To Investigate\\\" item and review logs, screenshots, and video recordings. Based on this comprehensive analysis, you can categorize this failed item. In ReportPortal there are 4 main defect types groups: Product Bug, Automation Bug, System Issue, No Defect. There is also the option to \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/reportportal-configuration/ProjectConfiguration#custom-defect-types\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"create subcategories\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In addition, during manual defect triage, you can take advantage of our ML-powered failure reason detection. If you’re not sure which category to put the bug in, use a hint from \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/MLSuggestions\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ML Suggestions\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". Also, pay attention to the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/SearchForTheSimilarToInvestigateItems\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"similar \\\"To Investigate\\\" items section\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", which displays test items with similar logs. In this way, the defect type can be applied to many test items at once.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Moreover, ReportPortal automatically groups tests by the same \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/UniqueErrorAnalysis\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"unique errors\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", which also speeds up test failure analysis.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to create defect in BTS\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"First, the team needs to determine which fields are required. Typically, when creating a bug (or defect/issue, which are the same thing), the following fields are filled in: summary, steps to reproduce, estimate, severity, priority, and attachments.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It is also recommended to link the bug to other work items, like user stories or requirements, so it is not isolated. This way, you can more efficiently pinpoint the root of the problem, determine when it occurred, and identify potential causes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What is the bug triage meeting in the scrum process?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This is a session where bugs are reviewed and discussed, typically with the participation of customers, developers, and testers. During a bug triage meeting, each bug is analyzed in detail, and its severity and priority are determined. If it is clear who will handle the bug, it might be assigned to that person during this meeting. If it's not, the bug is assigned to either the Backend or UI team.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"As a result, there are no uncategorized bugs left in the backlog. This greatly helps in the future, allowing for a swift understanding of what needs to be fixed first and what can be addressed later.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What's the difference between severity and priority? The bug's priority is assessed based on its importance for a specific version. For instance, during a bug triage meeting, discussion can be held about when this bug fix should be ready. If there's a bug that the customer wants to be fixed as quickly as possible and in the most recent version, it will be given the highest priority. On the other hand, severity is a measure of how critical a bug is to the application's operation. For example, if a login button looks different from the design, it doesn't crash the application, so the severity of such a bug would be low. However, its priority would be high because it's visible on the homepage and presents an aesthetic issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Regular bug triage meetings can help avoid scenarios where only the reporters are aware of the identified bugs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What are the benefits of the defect triage process in agile?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Effective workload planning in the team.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The tasks can be distributed based on what needs to be fixed first and what can wait.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Risk Management.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Through the triage process, defects are evaluated according to their severity and the impact they have on the software. This allows for the reduction of the likelihood of serious issues.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Improved Communication.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If a stakeholder is present at a bug triage meeting, the expected results for bugs with unclear requirements can be discussed immediately to prevent the team from having to redo everything multiple times later.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"After defect triage, testers can build a \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/blog/how-to-create-a-qa-metrics-dashboard-in-reportportal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"QA metrics dashboard\",\"marks\":[{\"type\":\"underline\"}],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". This dashboard provides a visual representation of all bug-related statistics: the total number, how many Product Bugs/Automation Bugs/System Issues there are, how many bugs are in a specific version, and the project's most problematic area.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In conclusion, ReportPortal can significantly facilitate defect triage by enabling comprehensive analysis and categorization of failures. Defect/Bug triage is a crucial step in software testing. By systematically categorizing and prioritizing reported bugs, teams can streamline workflows, enhance communication, and ultimately contribute to the successful delivery of high-quality software products. \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Defect triage with ReportPortal"},"leadParagraph":{"leadParagraph":"Defect triage or bug triage is a process of reviewing, categorizing, and prioritizing the reported bugs. The failure categorization plays an important role in the defect triage process as it helps to determine the type of failure. With ReportPortal, you can easily group the defects into categories."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/2AhQ8yZzrVvjvn4iSigs0I/5adfbf1a35023dd1f7d983d4becc9d70/DefectTriage-icon.png"},"description":""}},{"id":"bf3b04fc-485a-57d9-9417-491c33675fe0","slug":"how-to-create-a-test-automation-metrics-dashboard-in-reportportal","date":"March 22nd, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Typically, a test automation engineer uses the main metric of test coverage, which shows how many tests are written. However, this metric does not consider the quality of the tests. ReportPortal will assist you with this. With our help, you can build a test automation metrics dashboard to achieve a balance between speed, reliability, and relevance in automated testing. This dashboard is specifically focused on automation metrics, not quality metrics like the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/blog/how-to-create-a-qa-metrics-dashboard-in-reportportal\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"QA metrics dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In ReportPortal, you can create two versions of a test automation metrics dashboard. The first one is Launch level, when we focus directly on a certain set of tests or on a specific execution. For it, we build a history and collect metrics. We gather information for launches, showing information on how a particular Launch is progressing, and all the metrics above it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The second version of the test automation metrics dashboard involves aggregated widgets. Essentially, this dashboard centers around a unifying object: a build, a sprint, or a release.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rLaunch level dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rInvestigated percentage of launches\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows the number of items in launches with a defect type “To Investigate” and corresponding subtypes.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If there is a tendency not to deal with failed autotests, it means they are not useful.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Either automation engineers understand in advance that these errors are not serious, and it is not necessary explore “To Investigate”. Then the question arises, why these autotests were written, which do not make significant checks. After all, autotests should be of maximum benefit, all failures should be dealt with to indicate some kind of errors.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Therefore, with this widget, we can be confident that the team has reviewed and categorized all failures.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3fGMvrb1NC1E55K6tRufDi\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rLaunch statistics chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAs soon as this categorization appears, we get a new level of information – a distribution of failure reasons. We can simply look at the number of Failed and Passed tests. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"70NVNN3fiBLdXNCce6Jbz0\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rOr we can use ReportPortal's capabilities for categorizing failure reasons. This way we can understand the reasons why the tests didn't pass, and how many tests relate to each failure reason category.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"There are three main groups of failures: Product Bugs, Automation Bugs, and System Issues.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"System Issues. This is a category of problems related to technical debt of the infrastructure. One could say it is a DevOps debt, that is, the debt of the engineering team in setting up the infrastructure for flawless automation work.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What relates to the Automation Group – this is the technical debt of the automation team to fix all the problems, make automation stable, make locators stable, perhaps, update test suites.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Product Bugs with all its subcategories are those very test cases that help make the product better.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"NGAV3P6K3FN7Dg3Y2xZdb\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rFlaky test cases table (TOP-50)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows 50 unstable tests in the specified launches. When setting up, you can specify whether to consider before and after methods.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget helps to identify unstable autotests that pass checks every time with a different status. Next, we need to understand: maybe it’s not the product itself, but the autotest? For example, there is no response from the server, and the test fails because there was no update on request.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"One flaky test may not be a big problem, but several of them can certainly spoil the process. For example, if we then want to build Quality Gates that trigger automatic deployment after a health check, flaky test cases will hinder this capability.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It is recommended to fix the flakiest tests first, or even exclude them from the general run for some time.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"35jGrv2gqnIfAfC1dW2kRZ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Launches duration chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget shows the duration of the selected launches.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It’s best to build a filter for this widget based on launches of a certain direction, for example, regression, and against one environment. Then we can assess how automation is working and whether there is a scope of tests that stand out from the overall picture. Some launches may take too much time. For example, in the picture below, one launch lasted almost an hour. Next, we can look at the autotests in the problematic launch: maybe there is a test that takes a long time and the whole run takes a lot of time because of it. Perhaps it will be faster to carry out this check manually? Or, perhaps, it would be better to divide this large scope into several to speed up? Sometimes there is simply a lot of everything in one scope, and this is not always justified.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Some projects initiate extensive runs over the weekends, but good automation should still prioritize relative speed.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"AJrrcyzeCoqgCTxv0U4sr\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rTest-cases growth trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis chart shows how many new test cases have been written in each run. It allows observing if the team generally adds or excludes something from one run to the next.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4PtoF1YRSiHclhkJdzIWEn\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rMost failed test-cases table (TOP-50)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAnother important metric. If these tests fail for reasons other than Product Bugs, it means they are not testing the application, but are failing for some other reasons. It's necessary to verify why they failed and fix it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4YCZGgJyrsf0raHjFtM6Sf\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Failed cases trend chart\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget will allow to understand whether the team is moving towards reducing the causes of failure or increasing them.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"74a3nDOqrCmmRsZ9TzQwxb\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rMost time-consuming test cases widget (TOP-20)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rShows test cases that take the longest time in the last run of the specified launch.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It's convenient to use it in combination with Launches duration chart: as soon as we see that the launch takes a long time, we build Most time-consuming test cases widget by the name of this launch and look which test cases take the longest.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In the example below, the longest test case takes almost 8 minutes. We should think about whether we can somehow reduce it. Or, if this test always passes, we can think about how relevant it is and whether such a check is needed. Perhaps this is already a stable feature, and the autotest no longer has much value, and then we can remove it from the scope and save 8 minutes on the run. And if the test takes a long time, and periodically fails, then we need to figure out what's wrong with it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5PQIrXStlSmpl5I7faERyE\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rAggregated level dashboard\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rCumulative trend chart \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget allows you to collect all launches related to a particular build, release, or version into a general data array and show how many Passed/Failed tests there are and the reasons for their failures.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"For example, against the nightly build there were automatic jobs run that were related to API tests, integration tests, and end-to-end tests. We're interested in looking at the overall summary of the build, and how it passes. Not looking at individual jobs but looking at the summary of all results.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget can collect values by some attribute and show them over time. Accordingly, this way we can see how the stability of tests is improving. In the screenshot below, you can observe that initially most tests failed. The team started to add new test cases and stabilize the tests. At the second stage, you can see the emergence of System and Automation issues. Then, System Issues were fixed, remaining are Automation and Product Bugs, and so on until all failures decreased to a minimum value.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"zJoyno23jC1fYsvqPpPjY\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rOverall statistics\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThe aggregated mode of this widget also can collect a summary of all launches that can relate to a build, a team, or any other object that we group.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Separately, you can look at the distribution of failure reasons.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6MF74Ay2gUiJ48oboFpAdv\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rTable Component health check\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget allows you to view the state of the components we have tested. It can also look at the results of a specific build or selection.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget takes all the launches that fall into this selection, combines them into one common set of data, and subsequently can slice them by the attributes of the test cases that are in these launches.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In this case, at the first level are the features that are being tested. At the second level – split everything by priorities. At the third level – split by operating systems. At the fourth level – split by scenarios. Here you can add up to 10 levels. Including, you can add a custom column to see, for example, where the tests were run.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In this case, the features filtering, statistics, permissions, re-tries were tested. According to the settings, the Passing rate should be at least 70%, so the permissions component got a Failed status: out of 5 tests 3 tests failed, and they all relate to Product Bugs. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4sOpwUf6e9XvEOBgGTp3k7\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rIf we go deeper, we see that our tests are failing for critical functionality, medium and minor. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"43Y36MGom9aibIQgzp7CZY\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rWe go deeper, we see a distribution between Android and iOS. We go deeper and see that the user assign scenario is failing. We can go further and look at the selection of failed tests.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5oY5rXx71kjOawFsxFXvSh\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The widget gives us the possibility of granular viewing of all components and types of tests that were run. You can closely examine each specific component or critical functionality that has been tested. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-3\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Project activity panel\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThis widget shows what was done on the project.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This widget is useful for the understanding who is working on what. Sometimes you go to a launch, and the defect types have already been set, some items are missing. You can go to the Project activity panel and see who worked with this launch.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Also, when building a widget, you can specify a user name and see the actions of someone specifically. For example, it is visible when the user launched the launch, when he finished it, when he did the import, when he created widgets/dashboards, or some project integrations, linked an issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1vfV1HOZy6vRfbl6wR5Nqn\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Regular review and improvement of test automation metrics help identify areas that need enhancement. This process streamlines test automation workflows and provides insights for decision-making. Thanks to these metrics, organizations can deliver high-quality software products and remain competitive in the market. \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"How to create a test automation metrics dashboard in ReportPortal"},"leadParagraph":{"leadParagraph":"Typically, a test automation engineer uses the main metric of test coverage, which shows how many tests are written. However, this metric does not consider the quality of the tests. ReportPortal will assist you with this."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/3vgMLnroiqlSxLyorXOpnM/832dda2f718570fa97a41793be29ece9/TestAutomationMetrics-icon.svg"},"description":"Create test automation metrics dashboard in ReportPortal"}},{"id":"7e0cf4b3-af8b-561e-8707-dda2d9ccaa71","slug":"reportportal-completes-soc-2-type-ii-audit","date":"February 27th, 2024","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We are pleased to announce that ReportPortal has successfully completed a SOC 2 Type II audit performed by Deloitte Auditing and Consulting Ltd. We have confirmed that we accurately, comprehensively, and thoroughly control many aspects related to the provision of our service and the development of our product. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rThe examination was conducted throughout the period from May 1, 2023, to October 31, 2023. Deloitte performed the engagement in accordance with SOC 2, a compliance standard for service organizations developed by the American Institute of CPAs (AICPA). This standard is based on the following Trust Services Criteria: \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Security, Availability, Processing Integrity, Confidentiality, and Privacy\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\". Meeting SOC 2 requirements indicates that an organization maintains a high level of information security. Customers often require SOC 2 compliance from organizations, especially when choosing a SaaS provider, to ensure their data is protected. As part of the SOC 2 certification scope, our SAAS service was audited.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5rH8qLhsRP2aaWtVCccTE5\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Deloitte noted that ReportPortal, as a part of EPAM Global, has:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"unordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\rA \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Corporate Internal Audit Function\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", with Internal Auditing as an independent function aimed at enhancing risk management and operations.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"EPAM’s Process Asset Management\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", ensuring administrative and operational controls for each major functional area are documented in various policies, standards, and process descriptions.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Risk Assessment and Business Impact Analysis\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" procedures, integrating risk management into its operations via strategies such as annual security risk assessments and a Business Continuity Program to protect corporate assets and maintain service levels.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Operational Monitoring\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", where exceptions in normal or scheduled processing due to hardware, software, or procedural problems are logged, reported, and resolved daily, with appropriate levels of management reviewing these reports and taking action as necessary.\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Client Account Monitoring\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\", wherein each client is assigned to a Customer Success Manager who communicates regularly to discuss issues and client satisfaction.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"As a result of certification, Deloitte released SOC 2 and SOC 3 reports that you can \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/contact-us/general\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"request\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\\r\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Moving forward, we plan to go through this procedure annually to ensure we are doing everything correctly, with quality and proper control, to provide our clients with the best service.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"About ReportPortal\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\r\\nReportPortal is an open-source solution that provides a test automation reporting dashboard, ensuring transparency and efficiency across the DevTestOps pipeline. This comprehensive tool simplifies the test automation process and quickly identifies weak areas. It lowers costs and integrates seamlessly with the most popular frameworks, all powered by a real-time dashboard.\\r\\r\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Using ML, ReportPortal analyzes test outcomes, pinpoints the causes of failures, and assists in the defect triage process. As a SaaS, it minimizes the challenges of deployment and maintenance, while delivering all advantages of the product, efficaciously showcasing product readiness to management teams.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"ReportPortal completes SOC 2 Type II audit"},"leadParagraph":{"leadParagraph":"We are pleased to announce that ReportPortal has successfully completed a SOC 2 Type II audit performed by Deloitte Auditing and Consulting Ltd. "},"category":["Product"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4pRucyuhjD3S1x7dDWtS40/6c1e4cc062e0e87d604f78f26a3f0911/Property_1_SOC2.svg"},"description":"ReportPortal completes SOC 2 Type II audit"}},{"id":"197bd3c7-8718-5202-b4b4-e7f87bbd711d","slug":"flaky-test-best-prevention-practices","date":"November 21st, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky tests are the wild cards in the software testing world. Flaky tests happen when a test often changes its status from passed to failed and vice versa under the same conditions. Flaky tests can cause a lot of delays, questions, and chaos for software developers and QA teams.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky test cases table widget \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal allows to find flaky tests in your runs. For that, create a Flaky test cases table widget in our test automation results dashboard: specify the Launch name and the Launches count for comparison.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"blueBg The widget is built by the name of the launch, and not by the filter. To obtain all the data in this widget, make sure that the launches in which you want to address flaky tests have the same name.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Flaky test cases table widget shows the 50 most unstable test cases in the selected launches. This includes not only Failed tests but also tests that change their status from Passed to Failed, from Failed to Passed. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This widget is dynamic, it only needs to be built once, and it will update automatically.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6wnzPPQQjr0Mj5OFxl3u9G\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"When creating the widget, we can also specify whether to include Before and After methods. Before and After methods are preconditions/postconditions, for example, creating test data, and then, after running the tests, cleaning them up. The reason for flakiness is not always within the test itself – the methods can also be flaky. For instance, the test itself might fail if generating test data fails. In the Launch, the reporting may be fine, and the test case would have passed if the Before or After method was functioning correctly.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Therefore, when a certain number of runs have accumulated, especially in the case of large launches, it is challenging to analyze failed automated tests. You can create this widget, investigate the unstable tests, and understand if there is an issue with the Before/After methods. The problem may not be in functionality, but in automation. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Additionally, with the flaky test widget, you can identify issues with the environment or with a specific version or branch. For example, if a test runs on different environments and passes in some, fails in others, you can temporarily remove these tests from the scope until the environment-related issue is resolved, saving time on running and analyzing flaky tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"How to avoid flaky tests?\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\nFor those seeking ways to prevent flaky tests, consider the following best practices:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Stable Test Environment\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Check the availability of critical resources for test execution. Control external factors (e.g., system resources, network connectivity).\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Test Isolation\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Decrease the number of connections between tests. It can significantly minimize the risk of instability.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Optimal Test Timing\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Properly plan your tests to ensure optimal performance for your testing environment. Remember about network congestion and system load.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Test Data Management\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Do not use mutable or shared data that can cause flakiness.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Overall, if you know what causes flaky tests and apply best practices to prevent them, you can reduce the impact of flaky tests. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Flaky test: best prevention practices"},"leadParagraph":{"leadParagraph":"Flaky tests are the wild cards in the software testing world. Flaky tests happen when a test often changes its status from passed to failed and vice versa under the same conditions."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4eQDfyCFkL2RPwYbGorwjN/f8c1371ff85238ec12c9636e7ce82110/Flaky.svg"},"description":"Flaky test: best prevention practices"}},{"id":"37584917-9729-5501-af19-4f5118a55c69","slug":"how-to-create-a-qa-metrics-dashboard-in-reportportal","date":"October 27th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Nowadays, test automation is an essential part of the software development process. For effective test automation, it is crucial to build a QA metrics dashboard. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"But firstly, we have to complete some prerequisites.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\rPreconditions\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"1. Build a testing pyramid for the project. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Specify the number of tests at each level that are planned to be automated and those that are already automated to understand the coverage.\\n\\nFor example, we can have the following suggested groups: A, B, C.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A - the most critical tests that we would like to automate with 80% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"B - we work on these tests as a second priority, aiming for 60% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"C - we expect 20% coverage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"unordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It is important to appropriately mark test cases in the project management tool (for example, Jira, Rally, Gitlab, Trello) - what is planned to be automated, and what is already ‘In progress’. \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"2. Add attributes to test cases. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can use attributes to specify the area to which the test cases belong to, for example, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'component: UI'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". Additionally, attributes can indicate the scope of these tests, such as \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'label: smoke'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". In the attributes you can also specify the environment where the tests are run, as well as the branch, plugins, and third-party systems. When the same scope of test cases is executed against different environments, versions, or with different plugins, it is beneficial to indicate these differences in the attributes.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6JxigKYbkzoH5suIMsSThF\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, we have completed the preconditions, and now we are ready to create the test automation results dashboard. We recommend creating different dashboards for various purposes: for regression testing, for specific versions, for unstable tests, etc.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1lolQnY68IuB2PDbouJOVC\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In ReportPortal, you can track test automation metrics using our widgets.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Widgets for tracking test automation\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Component Health Check\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can create this widget using attributes and see which components, functionality, platforms, etc. are not working correctly so you should pay more attention to them.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3mFDnowm9naXOLAYOh8rPE\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Overall Statistics\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This is a summary of test cases with their statuses. In the test result report, you can only set Total/Passed/Failed/Skipped. Additionally, this widget shows the number of bugs categorized by defect type: Product bugs, Automation bugs, and System issues or custom defect types. The widget has clickable sections, allowing you to navigate and view test failures and their causes. Afterward, post an issue in the bug tracking system (BTS) for all Product and Automation bugs or link the already created tickets to the failed items in ReportPortal.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4ni9dNIqFOWdhuWUv3x5EK\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\\rUnique Bugs Table \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This widget displays existing bugs created in the BTS. It enables us to track the issues within our product. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2ybWGW8dyXbLEid1nzACpI\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Passing Rate Summary \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Initially, it is sufficient to understand the proportion of Passed/Failed cases.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Here, you need to pay attention to the failures: are there any tests that are constantly failing and that don’t yet have a defect type?\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4egJWwjKdLnoh1FZXi3m40\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Flaky test cases table \\r  \\r\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The widget displays the most unstable tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It's not just a failure when we know, for example, that it's an automation bug. But if a test first passed, then failed, and then passed again, it means the issue is either in the test itself or in the environment. We need to investigate everything we see here.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3uHxxD6GW09TA6jIL1kshz\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Cumulative trend chart\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You only need to build it once to obtain launch summary statistics with one attribute. For example, if we have an \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'environment: dev'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" attribute, it's convenient to view statistics for this environment.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In the ReportPortal, there's a peculiar feature: if a launch has a certain attribute, and there are no attributes at the step level, then all tests within that launch are associated with that attribute.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Therefore, if we have an \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"'environment: dev'\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" attribute, the system treats all tests within that launch as if they also have this attribute. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4E7UzDHdqkjzOBI9U7Xvmf\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In summary, test automation metrics help to enhance test automation, and as a result, contribute to the creation of high-quality software.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"How to create a QA metrics dashboard in ReportPortal"},"leadParagraph":{"leadParagraph":"Nowadays, test automation is an essential part of the software development process. For effective test automation, it is crucial to build a QA metrics dashboard. \n\nBut firstly, we have to complete some prerequisites."},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4BXb5XyjHaWoKZDWt78h6n/42688cc418cd33231ce60a5ff154ce6f/metrics.svg"},"description":"How to create a QA metrics dashboard in ReportPortal"}},{"id":"ee314060-16fa-5339-ab90-d468224f1d4e","slug":"funny-situations-in-testing","date":"October 20th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Software testing can be a bit of a joke sometimes! You never know what’s going to happen! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We have collected comical stories to highlight the humorous side of the serious world of software testing. Let’s read! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Email Extravaganza \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On one project, automation test report was supposed to arrive in the inbox when test results came back unsatisfactory. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The team tested the functionality, and everything went smoothly. A couple of years later, new QAs joined the team. They decided to revisit the old test cases, maybe thinking the email server was on vacation. Testers began generating fake reports. And guess what? The client started getting emails with weird text! By evening, the client's inbox was overflowing with email mayhem! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It turns out that someone had accidentally hardcoded the customer's email address and forgotten about it. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, the crisis averted, and lessons learned! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1rJGd7Z5bqgNEOHaGmki5G\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Emoji Surprise \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On a typical day, testers asked their colleague for a link to the smoke test results.  \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The tester sent the link. But then, out of nowhere, some URL-ID keys turned into smoking emojis! Everyone tried to click the link with bated breath. But the link was broken, with emojis hidden inside. The team laughed. Our clever tester said, “As you see, it's a 'Smoke' test!” \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4JixwuKz2UcHbQa2CURP0h\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Unintentional Spam \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"On the project where a telehealth system was being built, the testers needed to test out things like user creation, appointment scheduling, and so on. They used random phone numbers and email addresses. But guess what? When they connected it to the SMS gateway, the “random” phone numbers turned out to be real, and actual people started getting SMS notifications about appointments with a venereologist! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"These unsuspecting individuals thought they were getting spam and started complaining. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The client company couldn't believe what was happening, so they urgently bought SIM cards for test purposes. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"79pGknQ2dIlXMqWt8rZFr6\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The “Phantom Mouse” \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"One day, a QA team was examining a video editing software that allowed users to edit, enhance, and export videos. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A tester encountered a confusing situation: the cursor glided across the screen, pressing buttons without any input. Could it be a ghost? But fear not! It was just a wireless mouse with low battery, sending out random signals, causing chaos in the video software. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Sometimes the solution is simpler than we imagine! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"53NCcaS2JUdFncFettu8O7\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Time-Traveling Timestamp \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"While testing a messaging app, a tester noticed that one of the timestamps showed a message sent from the future. Shocking! \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This caused a great deal of confusion among the developers, but they couldn't replicate this issue. The tester tried various combinations but eventually figured out that the date and time on the phone were set incorrectly. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A simple correction, and voila! Back to the present, my friends. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1ad0p9elvcKrjEpjqus8tS\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"These stories serve as a reminder that even in the serious world of software testing, there's always something to laugh about. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Funny situations in testing"},"leadParagraph":{"leadParagraph":"Software testing can be a bit of a joke sometimes! You never know what’s going to happen! \n\nWe have collected comical stories to highlight the humorous side of the serious world of software testing. Let’s read! "},"category":["Other"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/5txL6K9hal4rlrRHCCueI9/37e81c2db1753c52cd201370038cea3c/Funny.svg"},"description":"Funny situations in testing"}},{"id":"14d869b2-fd5b-5439-aa55-0ce0022eb505","slug":"Deserialization-issue-workaround","date":"September 16th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We would like to highlight that in version 5.7.5 of API Service and Authorization Service we have updated the dependencies.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Unfortunately, the same dependency found its way into different versions, causing a Java error that looks like this: \\\"AuthUtils$SerialUidReplacingInputStream: Potentially Fatal Deserialization Operation\\\" when serializing/deserializing a user class or token.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Note 1\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This issue has already been fixed in \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/releases/Version23.2\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"version 23.2\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\".\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"blockquote\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Example of a stack trace from logs:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"blueBg \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"italic\"}],\"value\":\"AuthUtils$SerialUidReplacingInputStream : Potentially Fatal Deserialization Operation.\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\"\\r\\n\\r\\njava.io.InvalidClassException: Overriding serialized class version mismatch: local serialVersionUID = 550 stream serialVersionUID = 520 \\r\\nat com.epam.ta.reportportal.auth.util.AuthUtils$SerialUidReplacingInputStream.readClassDescriptor(AuthUtils.java:105) \\r\\nat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1992) \\r\\nat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870) \\r\\nat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201) \\r\\nat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687) \\r\\nat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489) \\r\\nat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447) \\r\\nat java.base/java.util.TreeSet.readObject(TreeSet.java:524) \\r\\nat java.base/jdk.internal.reflect.GeneratedMethodAccessor226.invoke(Unknown Source) \\r\\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Everything is functioning well; however, since users are verified for all requests, including reporting, many errors are filling up the Service API log. To prevent this log from excessively cluttering the Docker, additional logging rules need to be configured in Docker's configuration.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We recommend using the following settings in the Docker compose file for the ReportPortal services containers:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"5IaEMQgny0x2KzRGQnZaC6\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Please make sure, that you configured the default logging driver by adding the following values to the \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"/etc/docker/daemon.json (create this file if it doesn’t exist):\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4pvFxCse0nwrptUwJAZM4J\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://gist.githubusercontent.com/ykyuen/d07d83481c24e9b7ae2d1604157e131a/raw/c23c846d54a19ef28c1c61cf6a381772dae192c4/daemon.json\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Here is the link to the raw version for a quick copy.\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Note 2\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This is a default recommendation from Docker, and you may need to adjust it to fit your project's specific requirements.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"If your installation is via Kubernetes, you don't need to worry about log rotation because Kubernetes already has it configured by default. However, Docker doesn't have log rotation enabled by default, and its JSON log format can quickly consume all available space.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We plan to update Docker compose in the future, so these log settings will be included in the compose file automatically. Until then, if you are using version 5.7.5 or 23.1 with Docker installed, please check how much space the logs are occupying and update the Docker compose file accordingly.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Default paths for checking logs: \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Linux:\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" /var/lib/docker/containers/ \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Windows:\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" path depends on the way you’ve installed Docker. Please, check \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://docs.docker.com/desktop/troubleshoot/overview/#check-the-logs\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Docker documentation\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\".\\r\\r\\n\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"blockquote\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Deserialization issue workaround"},"leadParagraph":{"leadParagraph":"We would like to highlight that in version 5.7.5 of API Service and Authorization Service we have updated the dependencies. Unfortunately, the same dependency found its way into different versions, causing a Java error when serializing/deserializing a user class or token. How can this be fixed?"},"category":["Product"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/3DHvdns0eyniI6qPnrckE3/b24dc5f1ce7668dbc16f65ac36445d3f/Deserialization.svg"},"description":"Deserialization issue workaround"}},{"id":"581a785f-a448-5d49-8c9a-5cac42dd4dda","slug":"tips-and-tricks-for-successful-ci-cd","date":"August 9th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The main goal of CI/CD is to reduce lead time. This is an important metric that shows how quickly a new feature goes into production. With perfect CI, this process can take just a few minutes. What do we need to deliver features with such speed? Here are some recommendations from the EPAM Test Automation community Mikalai Biazruchka, Oleksandr Halichenko, Yauhen Klimiashuk, Dzmitry Prakapuk. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Define a Git branching strategy and environment strategy \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Continuous Delivery/Deployment is the process of validating and delivering a product to pre-production and production environments. This process goes through the available environments. Git branching strategy and environment strategy are interrelated and define the product build process – from which branch to which environment. If the Git branching strategy and environment strategy are not coordinated, then, accordingly, the process of delivering the product to pre-production and production will not be transparent and may exclude the possibility of building a CI/CD process. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Define quality gates \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"A quality gate is a set of step-by-step quality checks that help determine if we can proceed to the next stage or not. Examples of quality gates in development include code review, different linters, vulnerability detection via Sonar, and unit tests. Examples of quality gates in automation include smoke tests, running all tests, or some tests on changed objects. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"By the way, \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\", as a continuous testing platform, has a premium \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://reportportal.io/docs/quality-gates/QualityGatePurpose/\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Quality Gates feature\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\".\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Choose the right CI tool \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The best option would be the tool that comes with the code repository: GitHub Actions for GitHub, Azure Pipelines for Azure DevOps, and GitLab CI for GitLab. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Implement CI/CD as soon as possible \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Ideally, after every developer’s commit, the entire range of checks should be started: static code analysis, unit tests, code style analysis, integration tests, and end-to-end tests, including UI and API. These checks are aimed at building a Continuous Delivery process where we not only collect (integrate) everything but also deploy somewhere or provide artifacts, such as docker images. In the ideal world, each commit should turn into a new deployment to production. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Learn the toolset of the platform you are working with \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Each platform has its own set of commands and tools that are used for CI/CD-specific purposes. It happens that those who work, for example, on .NET do not know the Command-line interface (CLI) very well. Accordingly, a person who does not know the CLI will not be able to create a full-fledged CI process for the platform in question. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Use static code analysis\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Each platform has its tools to evaluate code quality. You should not only install them but also pay attention to what they offer because this provides serious feedback that will help to improve the quality of the code and prevent serious errors. These tools are easy to set up and take little time, but their value is significant. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Consider the pipeline's execution time \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Developers expect the CI pipeline to run in less than 30 minutes. This is the best practice now. If the pipeline takes longer, you need to think about how to fix it. This is where caching can help. It speeds up the CI pipeline a lot since we don't have to rebuild dependencies. On the other hand, caching introduces a certain element of instability because the cache may be invalid or outdated, which can lead to unexpected crashes. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Use parallel jobs \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Using parallel CI/CD jobs increases productivity. If you can parallelize specific steps without losing the validity of the pipeline's feedback, then it always makes sense to do that. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Use detailed logging\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Additional logging helps not only with the development of the CI/CD pipeline but also with its maintenance. When something goes wrong, having granularly split steps inside CI/CD and the correct logging makes finding a problem in the pipeline much easier. It becomes easier to understand what is happening and how it is happening. For example, if we produce some artifact, it would be nice to properly name it so that it is clear what it is, why it is needed, and where it came from. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Use a pipeline-as-code approach \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The recommended approach is to create the pipeline in YAML format or the form of JSON or some kind of executable script: it is located next to the code; it is easy for versioning and easy to change. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Inexperienced engineers often use a graphical interface to create a pipeline. This works well, but when something needs to be changed, questions arise: who changed it, what was changed, and when. Besides, if something went wrong after some change, it is problematic to find this change and revert it. Nowadays, almost all major platforms support the pipeline-as-code format, which allows you to write pipelines in text form. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It may seem difficult at the beginning of using this approach. But once you get comfortable with the syntax, it gives you more flexibility, and becomes more robust. If you need to transfer the pipeline from one project to another, then it will be just copying the file. If a graphical interface is used, it will most likely need to be recreated from scratch, where something can be forgotten, lost or overlooked. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Collaborate with the development team \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Automation should be in cooperation with the development team. Then, it will be possible to build an effective delivery process and avoid gray areas. It is necessary so that the development team can always indicate what should be paid attention to. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Optimize the test set for each stage \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Automate as much as possible but define clear timelines for each stage (sanity – on push, smoke – daily, regression – every week, etc.). Generally, test design is very important to have a well-established CI/CD process. Poorly prepared tests can take a long time to run or check the wrong thing. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Optimize version control system (VCS) flows \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Minimize branching to avoid high efforts on the code merge process. Encourage teams to merge frequently to avoid lots of branches per engineer. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Keep a stable infrastructure for testing \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It is great when autotests are integrated into the CI/CD pipeline, and at the same time, it is important that the tests run quickly, and the results don't depend on the infrastructure. For example, if we have 8 threads but only 5 browsers available, then use cloud providers (BrowserStack, Mobitru, SauceLabs) to speed up your test execution via parallel execution. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Use centralized test reporting tools \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Test automation reporting tools have enough capabilities to analyze test results and provide clear reports to the stakeholders. It is very convenient to have everything in one place and allows you to get the green light earlier. This is also evidence that the Continuous Delivery process is effective. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"By following these tips, you can build effective CI/CD, streamline the development process, and improve software quality. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Tips and tricks for successful CI/CD"},"leadParagraph":{"leadParagraph":"The main goal of CI/CD is to reduce lead time. This is an important metric that shows how quickly a new feature goes into production. With perfect CI, this process can take just a few minutes. What do we need to deliver features with such speed?"},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/2oHVDlJakMfnbuLu3VlPe0/59ff51e65b7b15b2d901f461b40d7841/CICD.svg"},"description":"Tips and tricks for successful CI/CD"}},{"id":"674361dc-575e-5943-9fca-e6dfac42e9d9","slug":"store-more-clean-faster","date":"July 26th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We are glad to announce the changes in the logic of the cleanStorage starting from the version 5.8.1 of Service Jobs and API Service version 5.9.0.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Previously, it was only possible to clear 1 attachment per 1 request to the binary storage, and it was not possible to clear more than 500,000 attachments per 1 job execution. The current implementation allows to clean multiple blobs (200,000) per 1 request to the S3 storage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The chunk_size env variable remains for the service jobs (no changes in the deployment are needed). But under the hood, the job logic is splitting chunk_size value into fixed batches by 200k and processing deletion via a certain number of iterations, which is calculated based on chunk size. If chunk_size = 2 million, then the job will proceed with the deletion with 10 iterations and clean 200k attachments per iteration.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The new logic is 30 times faster for both S3 and MinIO binary storages\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"Haq4WOWQa84702B9xiU56\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Use case 1:\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" if chunk_size is set to 200,000, the cleanStorage job will delete 200,000 MAX attachments from the S3 storage within 1 iteration. If the attachments count in the attacment_deletion table is less than 200,000, then all attachments will be deleted.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Use case 2:\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" if chunk_size is set to 2 million, the cleanStorage job will delete all attachments from the S3 storage within 10 iterations by batches of 200,000 attachments.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"1mJFEBfBHwoeQOZe488lu3\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"3ZfsczCXTD1mgOo4w6FJpl\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"Nz4JKSIXNW3WKJ6hPCWiv\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Thus, thanks to this implementation, we have significantly optimized the cleanStorage job performance. It keeps your binary storage within limits and allows you not to care about attachment cleaning. Since we've significantly speeded up the cleanup of binaries, we assume that this should completely solve all such problems.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"Store more, clean faster: CleanStorage job optimized for S3 and MinIO"},"leadParagraph":{"leadParagraph":"We are glad to announce the changes in the logic of the cleanStorage starting from the version 5.8.1 of Service Jobs and API Service version 5.9.0."},"category":["Performance improvements"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/UwooAbVwEvmwIRFhUng2c/329deb42838571300f551abc19ac7cd1/brown.svg"},"description":"Store more, clean faster"}},{"id":"a1d54a0e-7ed8-5378-9ab7-01179fe531d6","slug":"trends-in-automated-testing-in-2023","date":"June 22nd, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The GPT Gold Rush\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It's undeniable that the emergence of ChatGPT has rocked the entire world and all industries, particularly software development and testing. Large Language Models (LLMs) have been around for a while, but the implementation of such a generative version of an LLM model in a chat format, which maintains conversation and feels like an extremely well-informed companion, has captured people's attention even more.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Machine Learning models existed before the GPT-boom, but we've never identified them with such human-like qualities before. From generating high-quality text for your diploma or article to attempting to generate programming code – it has prompted people to reconsider how we write code or test applications.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"However, there's a subtle yet crucial line in how we use this tool. If we draw an analogy to the early 20th century, our attempts to use generative models mostly resemble efforts to ask the \\\"magic box\\\" – how to make horses faster, or how to optimally feed them on 1,000-mile journeys. In reality, we should be asking it or ourselves – what alternative modes of transport could we create and how can we create them?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Every company is now trying to find the most valuable use cases for generative models in their work, somewhere to replace routine, monotonous tasks, and in other cases, to completely create everything with them.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Unfortunately, these models are far from the concept of General AI and are not capable of doing all the work for us. But they can certainly assist us in areas of working with text, textual representation of steps, requirements. And even in generating basic actions and models in code.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Can ChatGPT write all the code for you now? It seems unlikely, at least for the time being. We can see this in the example of Co-pilot, which has access to almost all the codebases in the world inside GitHub, but still cannot write even a somewhat complex code structure. Perhaps because GitHub is filled with far-from-perfect code, 90% of which consists of examples from people learning technologies, and copied homework from each other, inheriting errors in them.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Therefore, for now, we can only talk about supplementing the development and testing process. And many companies in 2023 will be looking for optimal applications of such supplements. This will definitely continue into 2024.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"AI Augmented Testing\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Just as AR (Augmented Reality) occupies a space between the Real World and the Virtual (VR), so we're entering an intermediary stage between traditional testing and full-fledged testing with AI, which we'll call AI Augmented Testing. We'll inhabit this era until the advent of General AI or something closely resembling it.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"AI Augmented Testing could take various forms: from generating BDD scenarios based on existing keyword libraries or searching for the most similar test scenarios based on their steps, to generating templates for unit or API tests for any API in your application.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This will manifest itself in the emergence of both scripts and algorithms for local application, as well as extensions for code editors (IDEs). Even within our company, we've identified over 700 valuable applications of GPT-like models, sifted from a total of several thousand. The same process will undoubtedly be happening in other companies.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Are we just entering this phase? Far from it. We're already deep in it. Visual Testing with image recognition, like in \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://applitools.com/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Applitools\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", self-healing capabilities for Selenium-based test cases, like in \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://healenium.io/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Healenium\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", and the analysis and categorization of test results, like in \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/?utm_source=trigger&utm_medium=rp_1&utm_campaign=trends&utm_content=blog\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" - all of these are already here. Now they're receiving a new stimulus and renewed interest.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The trend for 2023-2024 will be the creation of accelerators capable of incorporating these AI Augmented Capabilities. The foundation for them should be platforms that cover the maximum testing or development workflow. Alternatively, this may lead to an increasing number of accelerators being compelled to amalgamate their abilities into a unified platform.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Data Privacy and Security\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Data protection is becoming even more critical. Some companies have globally prohibited the use of ChatGPT and its analogs due to concerns about data leaks or training models on company achievements. Therefore, local models like DaVinci and BERT will increasingly come into play.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The cost of these local models and the amount of data required for their training will likely be limiting factors. Thus, solutions that can serve as interfaces for any model, allowing the end user to choose which provides the best results, will take the lead.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Switching between local models and those from cloud providers will help achieve maximum results. 2023 and 2024 are set to be years of intensified development for local versions of these models, with a heightened focus on security.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Platform-centric Solutions\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Increasingly, market solutions and tools will move from niche solutions to horizontally expanding their capabilities. We're already seeing this with tools like SauceLabs and BrowserStack. These systems, initially providing access to remote browsers, are gradually broadening their scope through the addition of tools for performance, test result management, observability, test case management, visual testing, and more.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In the trends of 2023-2025, we likely anticipate a return to systems on the scale of HP ALM, but enriched with smarter features than before, augmented with AI. This could manifest in active company acquisitions or horizontal functionality growth.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"More SaaS and Less On-Prem\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Cloud solutions and services are permeating the development process more deeply, gaining trust from even the most conservative market players, such as banks and financial institutions. Development and system engineering, i.e., Dev and Ops, have almost entirely migrated to cloud services. It's now time for the Test part to follow suit and complete the DevTestOps cycle.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This trend will be strongly supported by the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.linkedin.com/posts/ghdmitry_ai-testing-software-activity-7050208879938281472-hChI/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Containerization Revolution I've written about previously\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". We've already learned to containerize production applications and we're doing quite well at containerizing development and testing environments. The next step is working with remote environments, which will, of course, be part of the SaaS infrastructure.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The trend for 2023-2024 will be an increased transition to SaaS systems, starting from Test Case Management systems to the execution of automated tests, use of remote infrastructure, test generation, result collection, and verification. Your computer will increasingly become a \\\"window\\\" into a large development infrastructure where you'll be writing some code executed somewhere in the cloud. And soon, you'll just be watching how this code is written by an AI algorithm and executed somewhere out there.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"JavaScript vs Programming Languages\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The trend for 2023-2024 will definitely be the continued exponential growth of test automation in JavaScript. Python is battling for the second position of steady growth, leaving JVM-based languages (Java, Kotlin, etc.) behind.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Based on the statistics from ReportPortal and the execution of tests involving our agents, we're witnessing the rapid growth of JS. This doesn't mean that Java automation has surrendered its position, not at all. Considering the age of this technology and the volume of existing automation in Java, it will remain in the top tier and even continue to grow for quite some time. However, the growth of JS-based automation is impressive.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In general, this is good news for the JS automation community, especially for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://playwright.dev/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Playwright\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". However, it's bad news for \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.cypress.io/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Cypress\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". Huge investments in marketing once allowed it to build its audience, but we're seeing interest in Cypress wane. Our suspicions are confirmed by \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.reddit.com/r/softwaretesting/comments/12ii8ib/cypressio_is_about_to_die_you_should_migrate_your/?rdt=46031\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"discussions in the Reddit community\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". It seems we're observing a \\\"sunset\\\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Consolidation and Persistence of Test Reporting\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Starting in 2021, projects have increasingly started to consider consolidated storage of testing results. This is especially considering the variety of test types, testing frameworks for their implementation, and even programming languages.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In 2023, the necessity and ability to gather all testing results together to make informed decisions becomes even more critical. Yes, the world at large, and testing in particular, will have a tough time fighting against the flood of Excel reports from team leaders. But in an era of increasingly cloud-based infrastructure and GitLab pipelines that don't leave behind artifacts, the ability to save results in real-time becomes more and more relevant.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/?utm_source=trigger&utm_medium=rp_2&utm_campaign=trends&utm_content=blog\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" has been addressing this capability for quite some time now. I'm reminded of questions at conferences about 5 years ago along the lines of \\\"Why do we need this?\\\" - now it's not just a possibility, it's a necessity. And we were able to catch this trend in time.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Quality Gates\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Starting in 2021, the possibility of creating automated Quality Gates has become increasingly relevant.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Imagine you've learned to gather all testing results at a single point, you've managed to conduct a complete test failure analysis and categorize their causes using ML algorithms, you have information about which components, flows, and priority parts have been tested, and now you'd like the ability to make an automatic decision on whether your application is ready to move further along the pipeline after the testing stage or not.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This is precisely what the concept of Quality Gates is for, which provides the ability to create complex rules for decision-making. For example, we don't want to have test failures for critical functionality, we don't accept product problems in functionality related to payment, but we're ready to pass the build if we have, for example, failures in test cases with minor priority and even more so if they failed due to test irrelevance.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"A premium feature called Quality Gates in ReportPortal allows you to achieve exactly this and provide the simplest Go/NO-GO answer back into your pipeline.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The trend for 2023-2024 will be the widespread adaptation of Quality Gates on top of the results of automated testing.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Shift-Left Testing\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"More and more major companies in the market are focusing on shift-left testing approaches. Namely, they're making testing tasks increasingly the responsibility of developers. Yes, there's still a need for specialized knowledge about the domain or specific testing conditions, which the testing team covers. But overall, there's a noticeable trend of moving automated testing closer to developers. What does this give companies? Primarily, it reduces expenses while improving product quality.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How can one increase quality while decreasing testing costs? The answer is quite simple - it involves breaking down silos in the development team's thinking, where the mindset is \\\"I've done my task, tossed it over the fence to testing, and it's no longer my concern.\\\" When the responsibility for quality becomes part of the developer's tasks, it changes attitudes towards how testable the code is and how resistant it is to exceptions and corner cases.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"One could argue that this slows down the team's velocity. And that's true. However, at the same time, it reduces the team cost for the testing and automation team. By changing the team composition, we can simply increase the number of developers on the team to maintain velocity.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"So, how do we save money then? If we've merely shifted costs from one team to another? The savings here occur due to accelerating the bug resolution cycle in the product. At the very least, there are fewer bugs, and at most, they're discovered faster at earlier stages, which also speeds up time to market.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In-Sprint Test Automation\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Emerging from the shift-left movement, in-sprint automation will gain more attention and interest as it will be a crucial and the most beneficial way to achieve a Shift-Left approach.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Trends in automated testing in 2023"},"leadParagraph":{"leadParagraph":"It's undeniable that the emergence of ChatGPT has rocked the entire world and all industries, particularly software development and testing. Large Language Models (LLMs) have been around for a while, but the implementation of such a generative version of an LLM model in a chat format, which maintain"},"category":["Test Automation Trends"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/3avlxQRB1mKpDmmeyhgVJX/b796046faa972850afddb9a80d4201ff/trends.svg"},"description":"Trends in automated testing in 2023"}},{"id":"82a36612-3c87-5e0a-b072-5cefd32d7aad","slug":"new-approach-to-versions-naming","date":"May 18th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Our team aims to release often and provide more flexibility for our users and ReportPortal team itself in feature implementation and delivery. Our priority is always to deliver the product to our users as soon as we can (without sacrificing the product quality). That gives us an opportunity to collect users' feedback earlier and adjust our development strategy following your needs and market trends.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Based on the above, we decided to review our versioning models and release flow and move toward the new ones:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Semi annual product releases\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Regular service release\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"ordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The previous approach to versioning\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Previously we used the approach to versions when the services were updated dependently. For example, if there were changes in the API service, Auth service and UI service that are not related to each other, we still made a release of these services trying to link them with a common version. Using this approach, we tried to understand in advance in which service and ReportPortal version the next changes will occur. We always synchronized all our components by the biggest change.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"With this approach, we often had to reschedule some features release when they were almost ready, but suddenly we urgently needed to update some component. Synchronization of services was causing problems, was painful and hard for the development process.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, we decided to change the approach to versions. Now we have replaced the Product release approach into separate Product release and Service release.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The main difference of a new approach\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"With the new approach, all our components (API Service, Auth Service, UI Service, Analyzer Service, Job Service) will have their own independent versions. We refused to synchronize versions. For example, if the API Service has major changes at the release time, and the UI Service and Analyzer Service have minor or patch changes, then their versions will be different. In addition, if there is no connection between their changes, we can release them separately upon readiness.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Product release\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The Product release will contain a set of new features and a set of different ReportPortal services’ versions. At the very beginning of Product delivery planning, we don’t know what versions of the components will be included in the next release.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We use \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://calver.org/\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Calendar Versioning\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" for Product releases. So, Product releases have the following pattern: yy.minor.micro(optional)-build, e.g., 2023.1 or 2023.2.5-768.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Service release\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Now we can release services separately from a Product release. Each service is independent of other services and has individual release and versioning. One important thing is to provide backward compatibility.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We use \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://semver.org/\"},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Semantic versioning\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\" for Service release.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"New approach to versions allows us to make fast implementation, testing, and delivery of new features to our users as well as collect and incorporate users feedback in our product. Besides functional changes this approach will allow us to release non-functional changes like security fixes in base image or dependencies more easily.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"New approach to versions naming"},"leadParagraph":{"leadParagraph":"Our team aims to release often and provide more flexibility for our users and ReportPortal team itself in feature implementation and delivery."},"category":["Delivery"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/6b5OGuNqI2tEtUG4RKib8X/3474e52252c47a470c2177893c6f3931/cvs.svg"},"description":"New approach to versions naming"}},{"id":"f4310872-4262-54c6-b864-3c4add2fccc2","slug":"How-we-use-AI","date":"April 13th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Over the last few years Artificial Intelligence (AI) has been changing the testing process in many ways. Robots are programmed to think. What is more, they do tasks at high speed and with accuracy, and they don’t get bored with it.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Let’s see how ReportPortal uses AI power for the key features: Analyzer, Unique errors, Machine Learning (ML) suggestions, Quality Gates.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"AI for shift-left testing\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"According to the shift-left testing approach, you should run automation tests regularly so that you understand what is happening as quickly as possible. For example, you can run a daily regression to check what happened to a product after a new code merge. Accordingly, with many runs, it will take a lot of time to analyze the test results. This is where AI comes to the rescue. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"You can train Analyzer, and then it will take a part of your test failure analysis routine work and set a defect type, a link to Bug Tracking System (BTS) (in case it exists), comment (in case it exists). How does it work? We can mark 1 of the bugs as a System Issue in 10 Launches and put it as a Product Bug in Launch 11. So, the Analyzer will mark this issue as a Product Bug the next time it is started. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"4YZgzTB3XHZ8ZGvfASMErg\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"blueBg \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Note:\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" The total count of tests should be the same every time for a specific Launch. Don’t skip the tests.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"If Auto-Analysis didn’t define any failures, you can open “Make decision” modal and see ML Suggestion for this item. AI will tell you that the error log is very similar to another log. You can compare these logs and apply the defect type from ML suggestion or set it manually. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"2ed74I1V5KBhOa5PcSK63L\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"One more feature with AI-based defects triage is Unique Errors. The system automatically groups tests by the same errors: when you expand some error log, you see a list of steps where it occurred. It is very convenient when preliminary work has already been done instead of you. Thus, you can select these grouped by AI items and apply a defect type for them using bulk operation. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"5yF6HMsAJGNHZxN5AGy3Ti\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"AI for CI/CD pipepline\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal speeds up a CI/CD pipeline thanks to Quality Gates feature, which has AI-driven “New errors” rule. In what situation can it be useful? Suppose you have already identified some errors, and they are minor, and you can go to release with them, and there is one more build to test. If you care about new unique bugs, you can create Quality Gate with “New errors” rule which works in conjunction with the Unique Errors functionality. Quality Gate will fail if a new defect is detected.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"“Amount of issues” Quality Gate rule is relevant to AI as well because Quality Gate with this rule is running after finish of Auto-Analysis. For example, if you have the rule “fail Quality Gate if there is at least 1 Product Bug”, and Auto-Analysis happens and marks an issue as Product Bug, then Quality Gate fails. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"7mbH9de0z31yw9704yO6Zj\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[],\"nodeType\":\"embedded-asset-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Overall, thanks to AI, you can get a test execution report and evaluate product health without any clicks. This magic process step by step:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Auto-Analysis is ON.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Quality Gates functionality is ON.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Integration with Jenkins is configured.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Launch is finished.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Auto-Analysis performs automated defect triaging and sets defect types.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal assesses Launch quality using the created Quality Gates rules.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"ReportPortal sends auto feedback to CI/CD tool with status Passed or Failed.\\r\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Based on ReportPortal feedback, CI/CD tool fails a build or promotes it to the next stage.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"ordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"AI enhances the value of ReportPortal by saving your time and resources on automation tests results analysis (consequently – reducing costs). AI highlights areas that require more testing and attention. This can help stakeholders quickly understand the quality of the product and make informed decisions.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}"},"title":{"title":"How we use AI"},"leadParagraph":{"leadParagraph":"Over the last few years Artificial Intelligence (AI) has been changing the testing process in many ways. Robots are programmed to think. What is more, they do tasks at high speed and with accuracy, and they don’t get bored with it."},"category":["AI"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4cbBtkShqpz0ixbNtoJNkr/91e23f61064b0d615f99d65fa64a04f0/technology.svg"},"description":"How we use AI"}},{"id":"0b2d47a4-9a4a-5321-a1a1-2b58b216c283","slug":"Tips-to-get-ReportPortal-benefits","date":"February 13th, 2023","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"ReportPortal is a one-stop solution to manage all your automation results and reports in one place. In this article our QA engineers shared their advice on how to use all ReportPortal capabilities to reduce test results analysis efforts and get pure visibility about product's health. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Use hierarchy in tests\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can run your tests on the Launch level only. But in this way, it's hard to understand what they refer to. Because all of them will have the same type.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Our advice is to use the next hierarchy in your tests in ReportPortal: Launch – Suite – Test – Step. For example, you can distribute your tests by areas using Suite level. Thus, it will be easy to recognize what functionality the failed tests belong to.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5XvIGCXKEv1kPG4c6OoXfL\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Use Test Case ID attribute\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The best way to distinguish test cases one from another is a Test CaseID. This is a fundamental attribute for the test case history and re-tries. If there is no Test CaseID defined explicitly the ReportPortal agent will generate it based on Code-Reference and parameters.\\n\\r\\nExplicitly Test Case ID can be added programmatically in code, via attributes: \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://github.com/reportportal/client-java/wiki/Test-case-ID\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"TestCaseID\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. Choose the relevant Launch name \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If you are running different types of tests (Unit, API, UI), specify this in the Launch name. Thus, it will be clear for the whole team: what is this Launch used for?  \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"4. Use attributes properly\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can describe the environment or version using attributes. What is more, you can filter test executions by these attributes. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Sometimes users define the owner of the launch via attributes as well. But we recommend another way: when running tests, use the Access token of the person who is responsible for this launch (it can be copied from the Profile page in ReportPortal). So, the specified user's name will be displayed as the owner. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"54cLIdQIrSoNnFn7o7DdLg\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"5. Configure reporting tests via CI/CD\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can run tests locally. But the best practice is reporting tests to ReportPortal via CI/CD pipeline (for example, Jenkins). \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"This way the whole team can run tests and view the results. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"6. Search for the similar \\\"To investigate\\\" items \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Most often you can see several \\\"To investigate\\\" items when reviewing the test results of the first run in ReportPortal. There are a few tips to make the test failure analysis easier. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Open \\\"Make Decision\\\" modal for an item with \\\"To investigate\\\", wait for \\\"Apply for...\\\" section to be fully displayed. Now you can see all \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/SearchForTheSimilarToInvestigateItems/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"the similar \\\"To investigate\\\" items\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". You can select all identical failures and perform the bulk operation for them. It speeds up test results analysis noticeably.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"7BR8bFmfXdCmjEF5ozEEkd\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"7. Use Analyzer and ML suggestions \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can delegate a part of the routine duties to the Analyzer. For example, you have 100+ failed tests. You can open every test and explore every test log to find the reason for failure. But it will take a long time. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"As an alternative, you can run \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/AutoAnalysisOfLaunches\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Auto-Analysis\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". Analyzer will find all known issues and will link corresponding defects based on your previous investigation results. After that, you only have to check (like a controller) whether everything was found by Analyzer.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"rYHnLnXGJj4w0GzU2rp1A\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If there are still some \\\"To investigate\\\" items left, just open \\\"Make Decision\\\" modal and look at the ML Suggestions. This functionality suggests the best options to categorize issues. It is a real time-saver for testers. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Note: Auto-Analysis and ML suggestions are useful in subsequent runs only after you do manual defect analysis in the first runs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5jVohved4Ovjvi5XiE1r6Z\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"8. Use Unique Errors Analysis\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"One more feature to facilitate tests results analysis is \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/analysis/UniqueErrorAnalysis/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Unique Errors\\\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Let’s imagine, you have 20 failures. Click on the Launch name and open the \\\"Unique Errors\\\" tab. Here you will find common errors for several failures. So, you can see 8 errors instead of 20, for example. Expand an error to check what tests belong to the same one. Then you can select some items and apply defect type/link issue/post issue for them via bulk update. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4Oq9r9Zr5yqVaRXHpMf9fC\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"9. Integrate ReportPortal with BTS\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Thanks to the integration with Bug Tracking Systems (Jira Server, Jira Cloud, Azure DevOps BTS, Rally) you will spend time once – when creating an issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Thanks to the integration with Bug Tracking Systems (\",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/plugins/AtlassianJiraServer\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Jira Server\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/plugins/AtlassianJiraCloud\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Jira Cloud\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/plugins/AzureDevOpsBTS/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Azure DevOps BTS\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/plugins/Rally/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Rally\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\") you will spend time once – when creating an issue.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1hATc39IU099Ml5x11N8Al\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"10. Use custom defect types\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"There are 4 main defect types in ReportPortal: Product Bug, Automation Bug, System Issue, and there is also \\\"No Defect \\\" group. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Custom defect types help to identify the most problematic area. For example, you have 5 Product Bugs, and you can specify each of them by functionality.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Another case: you can create \\\"Manually Passed\\\" defect type under \\\"No Defect \\\" group if the test is passed manually. It will help not to affect the overall statistic of the run. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Benefits:\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" to simplify the analysis, add the “Under Investigation” custom defect type or create custom defect type with the assignee name (for example, “Issue for Rob”). So other team members won't spend time analyzing such tests. This is about how to work in a big team without spending time on direct tasks assignments. Use comments as well for additional information or clarification on the issue, to better understand its context and potential causes. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"48zlHdzgO3A4em1H1YFBK6\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"11. Send manual tests in ReportPortal \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"There is a possibility to send tests from QaSpace (Jira plugin) to ReportPortal. It allows to get manual tests results as well and include them in the statistics via widgets.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To configure QaSpace with ReportPortal, copy Access Token from the profile on ReportPortal and use it as API Token for ReportPortal Configuration:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"17hBMtMXivk8d0p2qvr7ma\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Launches from QaSpace are displayed in ReportPortal like normal launches. The only difference is Test Steps are shown as a Log message.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5Gw2HlXuCESh2NCVfR9Xx8\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"12. Create widgets\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can create different widgets when all failures are investigated. Widgets show the application quality by area, component, environment on 1 Launch or several Launches. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can include manual tests from QaSpace in the widgets as well.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Widgets help to see the product's health. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The most popular widget is \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/dashboards-and-widgets/OverallStatistics/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\\"Overall Statistics\\\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\". You can use it (just by doing the screenshot or attaching a link to the dashboard in ReportPortal) in test results reports. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"3LM4ZuBTG5pLXcd7xmm7O\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"13. Use notifications\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"You can apply custom rules with specific conditions to send a notification on percentage of failed items which is very useful and helps to avoid unnecessary pings in case of random failures.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4weZeL2N9tUV08JI7ZcQMp\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"14. Use Quality Gates \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Let’s imagine an ideal test run: no Product Bugs, the percent of failure for all tests is 0 %. Create a \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/category/quality-gates/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Quality Gate\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" with these rules. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"While the Quality Gate is running, ReportPortal verifies testing results against the required conditions. It prevents the code from moving forward if it doesn’t meet the criteria.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4epJe2J9HLNX9TUSGkVWxc\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We hope these tips will help you improve your interaction with ReportPortal and get maximum value for your testing team and the entire project. \",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Tips to get ReportPortal benefits"},"leadParagraph":{"leadParagraph":"ReportPortal is a one-stop solution to manage all your automation results and reports in one place. In this article our QA engineers shared their advice on how to use all ReportPortal capabilities to reduce test results analysis efforts and get pure visibility about product's health. "},"category":["Best Practices"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/5nYxavPSP5Zvw9Mv9dKufl/1baafa70b037a6fd896f47689632d13e/insights.svg"},"description":"Tips to get ReportPortal benefits"}},{"id":"6d2f5e2b-1e53-5bb9-a1c9-dfc5e41fb8c6","slug":"performance-improvements-in-5-7-3","date":"December 22nd, 2022","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The system capacity has increased up to 13% compared to version 5.7.2\\n\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We are glad to announce some performance optimizations in version 5.7.3.\\n\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We updated the libraries in the scope of fixing Spring Framework Remote Code Execution (RCE) Vulnerability (Spring4Shell). As a result, the system capacity (requests per second) was increased up to 13% on the small server type* compared to version 5.7.2 during performance testing.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"* Check out the recommended \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/installation-steps/OptimalPerformanceHardwareSetup/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"optimal Kubernetes cluster configuration\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4OtiFoBv8fYe52yyMlYuep\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Benefits:\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" It helps to speed up your reporting on the same environment just because of the version upgrade.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Response time for building history and filtering at all levels has become at least 18% faster compared to version 5.7.2.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"5.7.3 version also brings performance optimizations in the core operations. The optimizations make basic functionality (filtering at all levels and test history building) at least 18% faster compared to version 5.7.2.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6mdsA3SM2DtuQQhxxZ8nUB\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Benefits: \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Test history and filters load faster compared to the previous version.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Check out the \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://reportportal.io/docs/category/releases\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"Release notes\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\" for the full list of what’s new in version 5.7.3.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"RabbitMQ version updates\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We updated the RabbitMQ version from 3.9.17 to 3.10.7 and faced less RAM and CPU usage of the RabbitMQ container under the high load (more than 1 million messages in the 20 queues total).\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"89UF4bzmja1XOf1En0mXQ\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"Xzwh2Ux20drHR7aof4elS\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"1M0mXfDr6RSmt4bTAl4uhx\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"2bzHPKmrH73XLtxIvCo1Lk\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"Benefits: \",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"It helps to decrease RabbitMQ resource utilization even under high workloads in the same environment just because of the version upgrade.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Thus, thanks to performance improvements in version 5.7.3 you can speed up your test reporting and decrease resources usage of the ReportPortal even under the high workload.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"With 5.7.3 version we are also addressing a huge list of security vulnerabilities. Please, see Release notes for details.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"Performance improvements in 5.7.3"},"leadParagraph":{"leadParagraph":"We are glad to announce some performance optimizations in version 5.7.3."},"category":["Performance improvements"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/4SDH3jaisqRiD81N8maID9/e91e990f7514198f1f6a172dba1e23be/report.svg"},"description":"Performance improvements in 5.7.3"}},{"id":"b3f0b221-339c-5923-9bc6-a813f60237d9","slug":"double-entry-in-5.7.2","date":"September 5th, 2022","author":"ReportPortal Team","articleBody":{"raw":"{\"nodeType\":\"document\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"The ReportPortal’s essence is based on assistance in working with automated testing results, and it all starts with the aggregation of results at a single place. For a long time, the relational database served well as a storage place for us. But the more test cases you run, the more storage you need to keep all related logs.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Before we introduced PostgreSQL with ReportPortal v5.0, we were using MongoDB from version 1 to version 4. MongoDB is a document-oriented non-relational database, was pretty good at writing the logs, but was a kind of nightmare to track metrics, build charts and find insights across testing results.  \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"PostgreSQL resolved data joins for us and made implementing new features much easier and quicker. But it also brought some drawbacks: such as excessive disk space usage to keep and rotate logs, slowdowns on data inserting, high CPU loads on data deletion and constant required maintenance (VACUUM clean).  \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"There are several reasons why \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://dba.stackexchange.com/questions/123627/postgresql-data-files-have-size-more-than-data-itself\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"PostgreSQL uses more disk space than the data itself\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\", and it comes mainly from indexes of the data and several copies of the same data, due to specifics of PostgreSQL mechanics. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Logs are a big portion of data inside ReportPortal. And assuming specifics of the workflow associated with it, it should be constantly added and deleted, without much of the need to keep it for a long time. Even ReportPortal’s Machine Learning in Auto-Analysis feature will use it just several times for training and will store it transformed inside the ML model. But constant insertions and deletions will produce a significant load and innumerous storage footprint. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"By reconsidering all pros and cons, we made a deep investigation and performance analysis of ElasticSearch vs PostgreSQL from the perspective of ReportPortal’s load model and nature of data. The results of this investigation go below in this article.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Assuming these results, our architectural decision was to switch logs storage from PostgreSQL to ElasticSearch, well… because we already have it as a part of our application. And since it gives us such benefits as reduced storage footprint up to 8.5x times, and data deletion up to 29x times quicker with just a fraction of the CPU load.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"And also, it opens up the capabilities of the full-text search!\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Please read our findings below.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"LFPLPVbLZAJlAeZdSvaSR\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Why we use ElasticSearch?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"PostgreSQL was previously used as a database for log storage, but – according to the performance tests – this is not the most effective way. Log messages take up the most space in the database, so we decided to transfer them to ElasticSearch. Logs migration to ElasticSearch will significantly reduce storage occupied by the log table. It will improve overall database performance (timings and costs of infrastructure). \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"In version 24.2 ReportPortal will still rely on ElasticSearch v7.10, as the last available version under Apache 2.0 license. And going forward we will consider Elastic-like solutions and forks, but this will come into play after a series of performance analyses and investigations. So please stay tuned, we know that it could be an issue for some of our users, but in order to avoid hasty decisions we have to make it incrementally. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6kpNmWJbj2X4sWUNHEcxAa\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg Due to the licensing policy chosen by Elastic, this has impacted the ability of some enterprise companies to use their solutions, including ElasticSearch. For some companies, this has become a turnover factor. Considering that it's in our interest to allow our clients to universally use ReportPortal without being restricted by the licenses of our underlying components, we are currently exploring alternative options.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What would you get with the ReportPortal v24.2 and the switch to the ElasticSearch logging? \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"It’s not something that you will see at the first moment, but definitely will benefit you in the long run, with outcomes like: \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"ordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Reduced disk space usage, with a smaller footprint up to x8.5. \\r\\r\\n\\r\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Reduced maintenance of the PostgreSQL database, and reduced requirements for the shape sizes by at least x2 times. \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Reduction of database load used by pattern analysis up to 5x times. \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Full-text search capabilities for text logs (x33 times quicker for text queries, and less CPU utilization 1 – 16x times in comparison with PostgreSQL). \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Similar performance with PostgreSQL on getting logs by ID. \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Storing logs in different indices per project allows to get project data faster and reduces the risks of locks occurrence.\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"5aW4VdMh36McTltHdmsbSz\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg ElasticSearch is lower by storage up to \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"8.5x\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" times.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Why we use Data Streams?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Elasticsearch provides a special approach for storing log data: “A data stream lets you store append-only time series data across multiple indices while giving you a single named resource for requests. Data Streams are well-suited for logs, events, metrics, and other continuously generated data,” – described in \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://www.elastic.co/guide/en/elasticsearch/reference/current/data-streams.html#data-streams\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"the official elastic search documentation\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Data Streams benefits \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"ordered-list\",\"data\":{},\"content\":[{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Logs deletion by IDs is x29 times faster in data streams compared to Indices;\\r\\n\\r\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Fast logs insertion (reporting) at the time of the high workload;\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Creation of cheap data nodes for old data, e.g., HDD with low resources. ElasticSearch allows configuring the old data storage using ILM (Index Lifetime Management) policy. It might be useful, for example, if your project uses some information once per week/month, etc;\",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"list-item\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Various index rollover conditions – fast creation of the new generation. It means that a new generation of this data stream is created when the limit is reached (by logs count, by logs amount, by date). So, logs of this data stream proceed to the new generation. Limits can be specified in the IML policy per project;\",\"marks\":[],\"data\":{}}]}]}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"6pdFzkHaqEkH0gybR2T4aR\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Response times table (95pct)\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"4c0jDiw0IcE9PMBhl0t2pU\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"heading-4\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Deletion by IDs performance comparison\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"embedded-asset-block\",\"data\":{\"target\":{\"sys\":{\"id\":\"43NYyP0KjWRtem1b2XVpkC\",\"type\":\"Link\",\"linkType\":\"Asset\"}}},\"content\":[]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"blueBg \",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"29 times faster\",\"marks\":[{\"type\":\"bold\"}],\"data\":{}},{\"nodeType\":\"text\",\"value\":\" in comparison with index, logs deletion by IDs from data streamers\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"29 times faster in comparison with index, logs deletion by IDs from data streamers \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://opster.com/guides/elasticsearch/data-architecture/elasticsearch-data-streams/\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\".\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"What effort is required from users? \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We recommend updating to version 5.7.2 for a smooth transition of full logging to ElasticSearch, especially if you have many logs. If you update to version 5.7.2, use it for 3-4 months before version 24.2. This period will be enough for the vast majority of projects to generate enough logs history inside ElasticSearch. And then update to version 24.2 once it is available. Since all logs will already be stored in ElasticSearch, no efforts will be required to do the migration. Along with version 24.2 we will distribute a migration script and instructions for data migration so that you can easily migrate from the early 5.x version. \",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"blockquote\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Note 1\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Before version 24.2, double logging might increase the resources usage – CPU, disk space. \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"blockquote\",\"data\":{},\"content\":[{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"Note 2\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"We are already using the ElasticSearch license, so, no new license is required. For now, we stay on version 7.10 with Apache 2.0. We might switch to OpenSearch in prospect.  \",\"marks\":[],\"data\":{}}]}]},{\"nodeType\":\"heading-2\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"How to enable or disable double entry?\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If you want to enable double entry, perform the following steps:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"1. Add ElasticSearch to your deployment (if you use Docker, you can check how to add it \",\"marks\":[],\"data\":{}},{\"nodeType\":\"hyperlink\",\"data\":{\"uri\":\"https://github.com/reportportal/reportportal/blob/master/docker-compose.yml\"},\"content\":[{\"nodeType\":\"text\",\"value\":\"here\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"text\",\"value\":\").\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"2. Add this variable to service-api and service-jobs:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"RP_ELASTICSEARCH_HOST=<your-host>\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"3. If you have authorization in your ElasticSearch, you also need to add these variables:\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"\\n\",\"marks\":[],\"data\":{}},{\"nodeType\":\"text\",\"value\":\"RP_ELASTICSEARCH_USERNAME=<your-elastic-username>\\nRP_ELASTICSEARCH_PASSWORD=<your-elastic-password>\",\"marks\":[{\"type\":\"code\"}],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"If you want to disable double entry, then you should remove all these variables from these services.\",\"marks\":[],\"data\":{}}]},{\"nodeType\":\"paragraph\",\"data\":{},\"content\":[{\"nodeType\":\"text\",\"value\":\"To summarize, using of ElasticSearch and Data Streams will bring significant performance benefits in the future.\",\"marks\":[],\"data\":{}}]}]}"},"title":{"title":"ReportPortal moves test logs from PostgreSQL to Elastic-type engines"},"leadParagraph":{"leadParagraph":"The ReportPortal’s essence is based on assistance in working with automated testing results, and it all starts with the aggregation of results at a single place. For a long time, the relational database served well as a storage place for us. But the more test cases you run, the more storage you need"},"category":["Architecture"],"featuredImage":{"file":{"url":"//images.ctfassets.net/1n1nntnzoxp4/60v9RHeDDNhpHIpea8nQ7s/131355e1407215723c900968e54e6a27/database.svg"},"description":"Moving logs from PostgreSQL to Elastic"}}]}},"pageContext":{}},"staticQueryHashes":["1130895138","2991584196","520980492"],"slicesMap":{}}